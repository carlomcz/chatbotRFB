{"config":{"indexing":"full","lang":["pt"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Vis\u00e3o Geral da ferramenta O Assistente Virtual de atendimento (chatbot) ir\u00e1 interagir com o usu\u00e1rio em uma conversa, onde o usu\u00e1rio vai digitar uma frase com a sua d\u00favida sobre um servi\u00e7o prestado pela RFB ou sobre alguma orienta\u00e7\u00e3o tribut\u00e1ria relativa a algum assunto de compet\u00eancia da Receita Federal do Brasil. O Bot ir\u00e1 procurar por uma resposta no Banco de Dados e enviar ao usu\u00e1rio a melhor ou as melhores respostas que encontrar, ou ent\u00e3o enviar uma mensagem de que n\u00e3o encontrou uma resposta apropriada. Neste \u00faltimo caso o Bot ir\u00e1 solicitar ao usu\u00e1rio a reformula\u00e7\u00e3o da sua pergunta ou ent\u00e3o a digita\u00e7\u00e3o de palavras chave para que o aplicativo fa\u00e7a uma busca no site da RFB com estas palavras digitadas, utilizando o mecanismo de busca do Google. Os Bancos de Dados com as perguntas e respostas foram criados a partir das informa\u00e7\u00f5es constantes no site da RFB, hospedado no dom\u00ednio .gov.br . Os dados foram extra\u00eddos utilizando-se programas feitos na linguagem de programa\u00e7\u00e3o Python. Ap\u00f3s a extra\u00e7\u00e3o gravamos os textos em planilhas Excel. Os textos extra\u00eddos foram processados , de modo a serem retiradas as palavras chave mais significativas, alimentando o \"Corpus\" dos modelos de dados, que ser\u00e3o utilizados para comparar as frases digitadas pelo usu\u00e1rio com as perguntas e respostas compostas a partir dos textos extra\u00eddos. A d\u00favida/pergunta do usu\u00e1rio ser\u00e1 tamb\u00e9m processada extraindo-se as palavras chaves significativas e depois ser\u00e1 comparada com cada uma das frases constantes no Banco de Dados, e o bot envia para o usu\u00e1rio as perguntas que tiverem os maiores \u00edndices de similaridade com a frase digitada. A conversa do ChatBot com os usu\u00e1rios foi implementada atrav\u00e9s de um aplicativo hospedado em uma p\u00e1gina Web, desenvolvido tamb\u00e9m na linguagem Python, utilizando-se do Framework Flask . Sobre o Trabalho desenvolvido Este aplicativo \u00e9 a base de um Artigo Cient\u00edfico que ser\u00e1 apresentado como requisito para obten\u00e7\u00e3o do t\u00edtulo de Especialista em Ci\u00eancia de Dados dos autores, como trabalho final do Curso de Especializa\u00e7\u00e3o em Ci\u00eancia de Dados - ECD, da Universidade Federal do Rio Grande - FURG. Autores Adriano S. Zumba, Carlo C. Taglialegna e Reynaldo Assun\u00e7\u00e3o Mar\u00e7o de 2021 RESUMO Este artigo se destina a propor o desenvolvimento de um assistente virtual (chatbot) que seja capaz de interagir com o contribuinte e direcion\u00e1-lo rapidamente ao servi\u00e7o demandado ou \u00e0s orienta\u00e7\u00f5es tribut\u00e1rias necess\u00e1rias, tendo em vista a grande quantidade de servi\u00e7os ofertados e os diversos canais de atendimento existentes na RFB. O objetivo do chatbot \u00e9 diminuir a dificuldade encontrada pelos contribuintes quando da demanda por um servi\u00e7o espec\u00edfico e evitar a navega\u00e7\u00e3o desnecess\u00e1ria pelas muitas p\u00e1ginas existentes no site da Receita Federal do Brasil. Palavras-chave: Chatbot; Atendimento; Receita Federal do Brasil. Nas se\u00e7\u00f5es seguintes ser\u00e3o detalhadas a metodologia e a implementa\u00e7\u00e3o do aplicativo proposto.","title":"Introdu\u00e7\u00e3o"},{"location":"#visao-geral-da-ferramenta","text":"O Assistente Virtual de atendimento (chatbot) ir\u00e1 interagir com o usu\u00e1rio em uma conversa, onde o usu\u00e1rio vai digitar uma frase com a sua d\u00favida sobre um servi\u00e7o prestado pela RFB ou sobre alguma orienta\u00e7\u00e3o tribut\u00e1ria relativa a algum assunto de compet\u00eancia da Receita Federal do Brasil. O Bot ir\u00e1 procurar por uma resposta no Banco de Dados e enviar ao usu\u00e1rio a melhor ou as melhores respostas que encontrar, ou ent\u00e3o enviar uma mensagem de que n\u00e3o encontrou uma resposta apropriada. Neste \u00faltimo caso o Bot ir\u00e1 solicitar ao usu\u00e1rio a reformula\u00e7\u00e3o da sua pergunta ou ent\u00e3o a digita\u00e7\u00e3o de palavras chave para que o aplicativo fa\u00e7a uma busca no site da RFB com estas palavras digitadas, utilizando o mecanismo de busca do Google. Os Bancos de Dados com as perguntas e respostas foram criados a partir das informa\u00e7\u00f5es constantes no site da RFB, hospedado no dom\u00ednio .gov.br . Os dados foram extra\u00eddos utilizando-se programas feitos na linguagem de programa\u00e7\u00e3o Python. Ap\u00f3s a extra\u00e7\u00e3o gravamos os textos em planilhas Excel. Os textos extra\u00eddos foram processados , de modo a serem retiradas as palavras chave mais significativas, alimentando o \"Corpus\" dos modelos de dados, que ser\u00e3o utilizados para comparar as frases digitadas pelo usu\u00e1rio com as perguntas e respostas compostas a partir dos textos extra\u00eddos. A d\u00favida/pergunta do usu\u00e1rio ser\u00e1 tamb\u00e9m processada extraindo-se as palavras chaves significativas e depois ser\u00e1 comparada com cada uma das frases constantes no Banco de Dados, e o bot envia para o usu\u00e1rio as perguntas que tiverem os maiores \u00edndices de similaridade com a frase digitada. A conversa do ChatBot com os usu\u00e1rios foi implementada atrav\u00e9s de um aplicativo hospedado em uma p\u00e1gina Web, desenvolvido tamb\u00e9m na linguagem Python, utilizando-se do Framework Flask .","title":"Vis\u00e3o Geral da ferramenta"},{"location":"#sobre-o-trabalho-desenvolvido","text":"Este aplicativo \u00e9 a base de um Artigo Cient\u00edfico que ser\u00e1 apresentado como requisito para obten\u00e7\u00e3o do t\u00edtulo de Especialista em Ci\u00eancia de Dados dos autores, como trabalho final do Curso de Especializa\u00e7\u00e3o em Ci\u00eancia de Dados - ECD, da Universidade Federal do Rio Grande - FURG.","title":"Sobre o Trabalho desenvolvido"},{"location":"#autores","text":"Adriano S. Zumba, Carlo C. Taglialegna e Reynaldo Assun\u00e7\u00e3o Mar\u00e7o de 2021","title":"Autores"},{"location":"#resumo","text":"Este artigo se destina a propor o desenvolvimento de um assistente virtual (chatbot) que seja capaz de interagir com o contribuinte e direcion\u00e1-lo rapidamente ao servi\u00e7o demandado ou \u00e0s orienta\u00e7\u00f5es tribut\u00e1rias necess\u00e1rias, tendo em vista a grande quantidade de servi\u00e7os ofertados e os diversos canais de atendimento existentes na RFB. O objetivo do chatbot \u00e9 diminuir a dificuldade encontrada pelos contribuintes quando da demanda por um servi\u00e7o espec\u00edfico e evitar a navega\u00e7\u00e3o desnecess\u00e1ria pelas muitas p\u00e1ginas existentes no site da Receita Federal do Brasil. Palavras-chave: Chatbot; Atendimento; Receita Federal do Brasil. Nas se\u00e7\u00f5es seguintes ser\u00e3o detalhadas a metodologia e a implementa\u00e7\u00e3o do aplicativo proposto.","title":"RESUMO"},{"location":"about/","text":"Sobre o ChatBot (a preencher!)","title":"About"},{"location":"about/#sobre-o-chatbot-a-preencher","text":"","title":"Sobre o ChatBot (a preencher!)"},{"location":"criando_modelos/","text":"Criando os modelos NLP utilizados pelo Bot Continua\u00e7\u00e3o do notebook jupyter: 02extrai_palavras_chave.ypnb : importando os pacotes gensim #!pip install gensim import gensim import gensim.downloader as api from gensim.models import TfidfModel from gensim.corpora import Dictionary from gensim import similarities Lendo dados para treinamento (servi\u00e7os e conversas comuns) # df1 = pd.read_excel('./ServicosRfbFinal2.xlsx') # servicos = [] for i in range(len(df1)): servicos.append(str(df1.iloc[i][4])) ids = [i for i in range(len(df1))] dataset1 = pd.DataFrame({'id': ids,'desc': servicos}) # df2 = pd.read_excel('./PerguntasTreinamento2.xlsx') # conversas = [] for i in range(len(df2)): conversas.append(str(df2.iloc[i][3])) ids = [i for i in range(len(df2))] dataset2 = pd.DataFrame({'id': ids,'desc': conversas}) Modelo Bag of Words(Bow) - Criando um dicion\u00e1rio, um corpus e um modelo # Transforma a descricao do servico em tokens de palavras input_tokens = [d.split() for d in dataset1['desc'].values] # Criando um dicionario com os textos do dataset dct1 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus1 = [dct1.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model1 = TfidfModel(corpus1) # # Transforma as conversas de treinamento em tokens de palavras input_tokens = [d.split() for d in dataset2['desc'].values] # Criando um dicionario com os textos do dataset dct2 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus2 = [dct2.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model2 = TfidfModel(corpus2) # # Salvando os modelos e os dicion\u00e1rios para uso do Chatbot # dct1.save('dct1') model1.save('model1') dct2.save('dct2') model2.save('model2') OBSERVA\u00c7\u00d5ES componentes do pipeline para criar o modelo (c\u00f3digo acima) input_tokens -> separa as palavras de cada texto; dct -> lista cada palavra dos textos em um dicionario; corpus -> faz um mapeamento de cada texto com os ids do dicionario (id da palavra, quantidade de ocorrencias); model -> \u00e9 o transformer responsavel por traduzir o corpus em um TF-IDF Criando uma matriz de similaridades At\u00e9 aqui fizemos um pipeline que transformou textos em matrizes TF-IDF e agora podemos comparar duas TF-IDFs e verificar sua similaridade. Ent\u00e3o iremos criar duas matrizes de similaridade com todos os textos dos Servi\u00e7os e das conversar comuns, que chamaremos de index1 e index2. Com as matrizes criadas poderemos comparar o texto digitado pelo usu\u00e1rio com os textos existentes em nossa base de dados. # Cria a matriz de similaridade dos Servi\u00e7os index1 = similarities.SparseMatrixSimilarity(model1[corpus1], num_features=len(dct1)) # Salva a matriz para uso no pipeline principal index1.save('index1') # # Cria a matriz de similaridade das conversas comuns index2 = similarities.SparseMatrixSimilarity(model2[corpus2], num_features=len(dct2)) # Salva a matriz para uso no pipeline principal index2.save('index2')","title":"Criando os modelos NLP"},{"location":"criando_modelos/#criando-os-modelos-nlp-utilizados-pelo-bot","text":"Continua\u00e7\u00e3o do notebook jupyter: 02extrai_palavras_chave.ypnb :","title":"Criando os modelos NLP utilizados pelo Bot"},{"location":"criando_modelos/#importando-os-pacotes-gensim","text":"#!pip install gensim import gensim import gensim.downloader as api from gensim.models import TfidfModel from gensim.corpora import Dictionary from gensim import similarities","title":"importando os pacotes gensim"},{"location":"criando_modelos/#lendo-dados-para-treinamento-servicos-e-conversas-comuns","text":"# df1 = pd.read_excel('./ServicosRfbFinal2.xlsx') # servicos = [] for i in range(len(df1)): servicos.append(str(df1.iloc[i][4])) ids = [i for i in range(len(df1))] dataset1 = pd.DataFrame({'id': ids,'desc': servicos}) # df2 = pd.read_excel('./PerguntasTreinamento2.xlsx') # conversas = [] for i in range(len(df2)): conversas.append(str(df2.iloc[i][3])) ids = [i for i in range(len(df2))] dataset2 = pd.DataFrame({'id': ids,'desc': conversas})","title":"Lendo dados para treinamento (servi\u00e7os e conversas comuns)"},{"location":"criando_modelos/#modelo-bag-of-wordsbow-criando-um-dicionario-um-corpus-e-um-modelo","text":"# Transforma a descricao do servico em tokens de palavras input_tokens = [d.split() for d in dataset1['desc'].values] # Criando um dicionario com os textos do dataset dct1 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus1 = [dct1.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model1 = TfidfModel(corpus1) # # Transforma as conversas de treinamento em tokens de palavras input_tokens = [d.split() for d in dataset2['desc'].values] # Criando um dicionario com os textos do dataset dct2 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus2 = [dct2.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model2 = TfidfModel(corpus2) # # Salvando os modelos e os dicion\u00e1rios para uso do Chatbot # dct1.save('dct1') model1.save('model1') dct2.save('dct2') model2.save('model2') OBSERVA\u00c7\u00d5ES","title":"Modelo Bag of Words(Bow) - Criando um dicion\u00e1rio, um corpus e um modelo"},{"location":"criando_modelos/#componentes-do-pipeline-para-criar-o-modelo-codigo-acima","text":"input_tokens -> separa as palavras de cada texto; dct -> lista cada palavra dos textos em um dicionario; corpus -> faz um mapeamento de cada texto com os ids do dicionario (id da palavra, quantidade de ocorrencias); model -> \u00e9 o transformer responsavel por traduzir o corpus em um TF-IDF","title":"componentes do pipeline para criar o modelo (c\u00f3digo acima)"},{"location":"criando_modelos/#criando-uma-matriz-de-similaridades","text":"At\u00e9 aqui fizemos um pipeline que transformou textos em matrizes TF-IDF e agora podemos comparar duas TF-IDFs e verificar sua similaridade. Ent\u00e3o iremos criar duas matrizes de similaridade com todos os textos dos Servi\u00e7os e das conversar comuns, que chamaremos de index1 e index2. Com as matrizes criadas poderemos comparar o texto digitado pelo usu\u00e1rio com os textos existentes em nossa base de dados. # Cria a matriz de similaridade dos Servi\u00e7os index1 = similarities.SparseMatrixSimilarity(model1[corpus1], num_features=len(dct1)) # Salva a matriz para uso no pipeline principal index1.save('index1') # # Cria a matriz de similaridade das conversas comuns index2 = similarities.SparseMatrixSimilarity(model2[corpus2], num_features=len(dct2)) # Salva a matriz para uso no pipeline principal index2.save('index2')","title":"Criando uma matriz de similaridades"},{"location":"extrai_links/","text":"Extra\u00e7\u00e3o dos Links das orienta\u00e7\u00f5es tribut\u00e1rias O arquivo 03scrapy_orientacoes.py a seguir , possui o c\u00f3digo python que extrai os links das orienta\u00e7\u00f5es tribut\u00e1rias existentes no Menu \"Assuntos\" do site da RFB, utilizando a biblioteca scrapy. Para cada um dos Assuntos existentes no Menu, roda-se o programa com a linha da vari\u00e1vel \"start_urls\" daquele assunto descomentada (sem \"#\") e com a dos outros assuntos comentada (com \"#\"). Substitui-se a cada execu\u00e7\u00e3o para um determinado assunto, na linha de comando, o nome do arquivo .xml que ser\u00e1 gerado com o nome de cada asssunto. A linha de comando ser\u00e1 ent\u00e3o: scrapy runspider 03scrapy_orientacoes.py -o 5_pagamentos.xml , onde neste exemplo, seria a linha de comando para o assunto \"Pagamentos e Parcelamentos\". O programa tem o c\u00f3digo reproduzido abaixo: # # para utilizar rodar o comando: scrapy runspider .\\03scrapy_orientacoes.py -o 5_pagamentos.xml, alterando-se o nome do arquivo xml de sa\u00edda a cada execu\u00e7\u00e3o para um assunto diferente # import re from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor from scrapy.item import Item, Field class MyItem(Item): url= Field() class someSpider(CrawlSpider): name = 'crawltest' allowed_domains = ['www.gov.br'] #1 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/restituicao-ressarcimento-reembolso-e-compensacao'] #perdcomp #2 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/certidoes-e-situacao-fiscal'] # certidoes e situacao fiscal #3 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cobrancas-e-intimacoes'] # cobrancas e intimacoes #4 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/declaracoes-e-demonstrativos'] # declaracoes-e-demonstrativos #5 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/pagamentos-e-parcelamentos'] # pagamentos-e-parcelamentos #6 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/tributos'] # tributos #7 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros'] # cadastros #8 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/julgamento-administrativo'] # julgamentos administrativos start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/julgamento-administrativo'] # julgamento-administrativo #substituir cada assunto na linha rules abaixo rules = (Rule(LxmlLinkExtractor(allow=r\"^https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/julgamento-administrativo\"), callback='parse_obj', follow=True),) def parse_obj(self,response): item = MyItem() item['url'] = [] for link in LxmlLinkExtractor(allow=self.allowed_domains,attrs=('href')).extract_links(response): item['url'].append(link.url) return item # Ao fim da execu\u00e7\u00e3o do programa para cada um dos assuntos , ser\u00e1 gerado um arquivo .xml com os links extra\u00eddos para cada assunto, que ter\u00e3o o seu processamento feito por uma rotina similar aquela descrita em se\u00e7\u00f5es anteriores.","title":"Extraindo Links das Orienta\u00e7\u00f5es"},{"location":"extrai_links/#extracao-dos-links-das-orientacoes-tributarias","text":"O arquivo 03scrapy_orientacoes.py a seguir , possui o c\u00f3digo python que extrai os links das orienta\u00e7\u00f5es tribut\u00e1rias existentes no Menu \"Assuntos\" do site da RFB, utilizando a biblioteca scrapy. Para cada um dos Assuntos existentes no Menu, roda-se o programa com a linha da vari\u00e1vel \"start_urls\" daquele assunto descomentada (sem \"#\") e com a dos outros assuntos comentada (com \"#\"). Substitui-se a cada execu\u00e7\u00e3o para um determinado assunto, na linha de comando, o nome do arquivo .xml que ser\u00e1 gerado com o nome de cada asssunto. A linha de comando ser\u00e1 ent\u00e3o: scrapy runspider 03scrapy_orientacoes.py -o 5_pagamentos.xml , onde neste exemplo, seria a linha de comando para o assunto \"Pagamentos e Parcelamentos\". O programa tem o c\u00f3digo reproduzido abaixo: # # para utilizar rodar o comando: scrapy runspider .\\03scrapy_orientacoes.py -o 5_pagamentos.xml, alterando-se o nome do arquivo xml de sa\u00edda a cada execu\u00e7\u00e3o para um assunto diferente # import re from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor from scrapy.item import Item, Field class MyItem(Item): url= Field() class someSpider(CrawlSpider): name = 'crawltest' allowed_domains = ['www.gov.br'] #1 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/restituicao-ressarcimento-reembolso-e-compensacao'] #perdcomp #2 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/certidoes-e-situacao-fiscal'] # certidoes e situacao fiscal #3 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cobrancas-e-intimacoes'] # cobrancas e intimacoes #4 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/declaracoes-e-demonstrativos'] # declaracoes-e-demonstrativos #5 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/pagamentos-e-parcelamentos'] # pagamentos-e-parcelamentos #6 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/tributos'] # tributos #7 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros'] # cadastros #8 start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/julgamento-administrativo'] # julgamentos administrativos start_urls = ['https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/julgamento-administrativo'] # julgamento-administrativo #substituir cada assunto na linha rules abaixo rules = (Rule(LxmlLinkExtractor(allow=r\"^https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/julgamento-administrativo\"), callback='parse_obj', follow=True),) def parse_obj(self,response): item = MyItem() item['url'] = [] for link in LxmlLinkExtractor(allow=self.allowed_domains,attrs=('href')).extract_links(response): item['url'].append(link.url) return item # Ao fim da execu\u00e7\u00e3o do programa para cada um dos assuntos , ser\u00e1 gerado um arquivo .xml com os links extra\u00eddos para cada assunto, que ter\u00e3o o seu processamento feito por uma rotina similar aquela descrita em se\u00e7\u00f5es anteriores.","title":"Extra\u00e7\u00e3o dos Links das orienta\u00e7\u00f5es tribut\u00e1rias"},{"location":"gera_orientacoes/","text":"Processa Links e gera Textos com as orienta\u00e7\u00f5es tribut\u00e1rias O arquivo 04gera_banco_orientacoes.ipynb a seguir , possui o c\u00f3digo python que processa os links das orienta\u00e7\u00f5es tribut\u00e1rias extra\u00eddos na etapa anterior e vai fazer o Web Scraping dos textos de cada um dos links e gerar o Banco de Dados com as perguntas e respostas de cada assunto e criar o modelo de dados que ser\u00e1 utilizado pelo ChatBot para responder \u00e0s d\u00favidas formuladas pelos usu\u00e1rios. # Rotina que l\u00ea os arquivos .xml com os links extra\u00eddos para cada assunto com Orienta\u00e7\u00f5es Tribut\u00e1rias # e os processa para gerar o banco de dados com as perguntas e respostas que ser\u00e3o utilizadas pelo ChatBot Importanto as bibliotecas necess\u00e1rias #IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS import pandas as pd import openpyxl import requests import urllib from urllib.request import urlopen import time from bs4 import BeautifulSoup import os import re from string import punctuation from collections import defaultdict from heapq import nlargest import nltk nltk.download('stopwords') nltk.download('punkt') from nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from nltk.probability import FreqDist Processa os links de orientacao tribut\u00e1ria para o assunto restituicao, ressarcimento, reembolso e compensacao # # processando os links de orientacao tribut\u00e1ria para o assunto restituicao-ressarcimento-reembolso-e-compensacao with open('1_perdcomp.xml', 'r') as f: #l\u00ea arquivo .xml data = f.read() Bs_data = BeautifulSoup(data, \"xml\") # transforma arquivo lido em objeto BeautifulSoup lnks = Bs_data.find_all('value') # encontra todos os links lista = [] #cria vari\u00e1vel \"lista\" para armazenar os links ix = 0 for i in lnks: #acrescenta \u00e0 lista somente os links do assunto correspondente if i.text[0:120] == 'https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/restituicao-ressarcimento-reembolso-e-compensacao': lista.append(i.text) # print(len(lista)) # Eliminando os links duplicados # pr\u00e9-processamento dos links, para eliminar links duplicados lista1 = [] for i in lista: ind1 = i.find('#') if ind1 != -1: st2 = i[0:ind1] else: st2 = i lista1.append(st2) # # len(lista1) # lista2 = [] for i in lista1: if i[-1] == \"/\": st2 = i[0:len(i)-1] else: st2 = i lista2.append(st2) # # len(lista1) # lista3 = [] # lista final com os links for i in lista2: if i not in lista3: lista3.append(i) # # len(lista3) Criando um DataFrame com os links e salvando em planilha # Cria um DataFrame com os links, em ordem alfab\u00e9tica # e salva em uma planilha excel dfx = pd.DataFrame({'Links': lista3}) # sorted_dfx =dfx.sort_values(by=['Links']) #Salvando para uma planilha sorted_dfx.to_excel('./1_Links_perdcomp.xlsx', index = False) # # carregando os assuntos para o DataFrame df1 = pd.read_excel('./1_Links_perdcomp.xlsx') df1 Acessa cada link para \"raspar\" o titulo e o texto sobre cada assunto # acessando cada link para \"raspar\" o titulo e o texto sobre cada assunto links1 = df1['Links'] links2 = [] servicos2 = [] htmls = [] oques = [] for i in range(len(links1)): try: if i < 0: print(\"#\", i) continue except: pass html = urlopen(str(links1[i])) print(\"#\", i) bs = BeautifulSoup(html, 'html.parser') #class=\"documentByLine\" id=\"plone-document-byline\" eConteudo = bs.find('div', id='plone-document-byline') titulo = bs.find('h1', class_='documentFirstHeading') #eConteudo = bs.find('h1', class_='documentFirstHeading') if eConteudo: print(links1[i]) try: div_txt = bs.find('div', id='content-core') texto = div_txt texto1 = div_txt.text except: div_txt = titulo texto = div_txt texto1 = \"*\" #print(texto[0:40]) links2.append(links1[i]) servicos2.append(titulo.text) htmls.append(texto) oques.append(texto1) time.sleep(3) ## Criando um DataFrame com os textos e salvando para uma planilha # salvando os textos para um dataframe dfy = pd.DataFrame({'Link': links2, 'Assunto': servicos2, 'Html': htmls, 'Texto': oques}) # #Salvando para uma planilha dfy.to_excel('./1_Perguntas_perdcomp.xlsx', index = False) Processa os textos para: remover as palavras desnecess\u00e1rias; selecionar as senten\u00e7as mais importantes; extrair as palavras-chave de cada texto # remove as STOPWORDS e processa o texto para selecionar as senten\u00e7as mais # importantes , para em seguida extrair as palavras chave # stopwords1 = set(stopwords.words('portuguese') + list(punctuation) + ['\u2013']) #stopwords = stopwords.words('portuguese') + list(punctuation) + ['\u2013'] plx = ['``',\"''\"] df1 = pd.read_excel('./1_Perguntas_perdcomp.xlsx') # chave_lst = [] for i in range(len(df1)): sentencas = sent_tokenize(str(df1.iloc[i][2])) texto = \"\" for j in sentencas: texto = texto + j + \" \" texto = texto.strip().lower() palavras = word_tokenize(texto) palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords1] palavras_sem_stopwords1 = [] for pal in palavras_sem_stopwords: if pal not in plx: palavras_sem_stopwords1.append(pal) palavras_sem_stopwords = palavras_sem_stopwords1 frequencia = FreqDist(palavras_sem_stopwords) sentencas_importantes = defaultdict(int) for k, sentenca in enumerate(sentencas): for palavra in word_tokenize(sentenca.lower()): if palavra in frequencia: sentencas_importantes[k] += frequencia[palavra] idx_sentencas_importantes = nlargest(5, sentencas_importantes, sentencas_importantes.get) str_chave = str(df1.iloc[i][1]+' ')*3+ (sentencas[0]+ ' ')*3 for y in sorted(idx_sentencas_importantes): #print(y,sentencas[y]) str_chave = str_chave + \" \" + sentencas[y] chave_lst.append(str_chave) Salva os dados em uma planilha # salva os textos em uma planilha # df1['texto_chave'] = chave_lst df1.to_excel('./1_Perguntas_perdcompFinal.xlsx', index = False) # len(df1) Todo o c\u00f3digo anterior utilizado para gerar a planilha 1_Perguntas_perdcompFinal.xlsx ser\u00e1 repetido para cada um dos outros 7 assuntos cujos links foram salvos em planilha, gerando ao final mais 7 planilhas iguais a esta planilha. Como o c\u00f3digo \u00e9 id\u00eantico, n\u00e3o ser\u00e1 repetido nem comentado novamente. Neste ponto consolidamos manualmente utilizando o pr\u00f3prio Microsoft Excel, as seguintes planilhas: - 1_Perguntas_perdcompFinal.xlsx; 2_Perguntas_certidoesFinal.xlsx; 3_Perguntas_cobrancasFinal.xlsx; 4_Perguntas_declaracoesFinal.xlsx 5_Perguntas_pagamentosFinal.xlsx; 6_Perguntas_tributosFinal.xlsx; 7_Perguntas_cadastrosFinal.xlsx , e 8_Perguntas_julgamentoFinal.xlsx Com isso foi gravada uma planilha de nome 0_Perguntas_orientacoes_Final.xlsx , contendo todos os links que constam em cada uma destas 8 planilhas. Seguindo com o processamento: Importa a biblioteca spacy # importa a biblioteca spacy e carrega o corpus do idioma Portugu\u00eas import spacy nlp = spacy.load(\"pt_core_news_md\") L\u00ea a planilha consolidada manualmente, com todos os links: # l\u00ea uma planilha consolidada com o conte\u00fado dos 8 assuntos gravados nas planilhas anteriores # A consolida\u00e7\u00e3o foi feita manualmente, utilizando-se o pr\u00f3prio Excel # df1 = pd.read_excel('./0_Perguntas_orientacoes_Final.xlsx') Importa a biblioteca nltk # importa a biblioteca nltk from nltk.stem import RSLPStemmer from nltk.tokenize import sent_tokenize, word_tokenize nltk.download('rslp') ste = nltk.stem.RSLPStemmer() Rotina que extrai as palavras chaves de cada pergunta/resposta Observa\u00e7\u00e3o: muito similar (quase id\u00eantica) as rotinas que processaram os servi\u00e7os no arquivo \"02extrai_palavras_chave.ipynb\" # rotina que extrai as palavras chave de cada pergunta/resposta, muito similar (quase id\u00eantica) # as rotinas que processaram os servi\u00e7os no arquivo \"02extrai_palavras_chave.ipynb\" def troca_acentos(txt): transTable = txt.maketrans(\"\u00e1\u00e0\u00e3\u00e9\u00ea\u00ed\u00f3\u00f4\u00f5\u00fa\u00c1\u00c3\u00c3\u00c9\u00ca\u00cd\u00d3\u00d4\u00d5\u00da\u00e7\u00c7\", \"aaaeeiooouaaaeeioooucc\") txt = txt.translate(transTable) return str(txt) # # # fun\u00e7\u00e3o que tokeniza as palavras # def tokenizar(texto): # # car_nok = ['SPACE', 'DET','ADP', 'CCONJ','PUNCT','PRON','SCONJ'] txx = texto docxx = nlp(txx) txx_lemma = [(token.text,token.orth_, token.lemma_, token.pos_) for token in docxx] pal_xx = '' plvxx = \"\" for i in txx_lemma: if i[3] not in car_nok and len(i[0]) > 2: plvxx = plvxx + ste.stem(i[0]) + ' ' plvxx = plvxx.strip() plvxx = troca_acentos(plvxx) return plvxx # fim funcao tokenizar # # fun\u00e7\u00e3o para limpar o texto def limpa_texto(txt_a_limpar): txt_ok = txt_a_limpar.lower() txt_ok = txt_ok.strip() return txt_ok # # fim da funcao limpa_texto # # # cria planilha com as palavras chave dos servi\u00e7os print(\"***inicio tokeniza\u00e7\u00e3o***\") plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df1)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df1.iloc[j][3] txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df1['chaves_serv'] = plvx1 # # print(\"####fim tokeniza\u00e7\u00e3o####\") # Salva os textos em uma planilha # salva para uma planilha excel df1.to_excel('./0_Perguntas_orientacoes_Final2.xlsx', index = False) # # l\u00ea as perguntas e respostas salvas na planilha e carrega para o DataFrame df1 = pd.read_excel('./0_Perguntas_orientacoes_Final2.xlsx') # Importando a bibliote gensim # importa a biblioteca gensim #!pip install gensim import gensim import gensim.downloader as api from gensim.models import TfidfModel from gensim.corpora import Dictionary from gensim import similarities Cria um DataFrame com as palavras-chave # cria uma lista com as palavras-chave de cada pergunta/resposta servicos = [] for i in range(len(df1)): servicos.append(str(df1.iloc[i][5])) ids = [i for i in range(len(df1))] # # cria um DataFrame com a descri\u00e7\u00e3o e o id de cada pergunta dataset1 = pd.DataFrame({'id': ids,'desc': servicos}) Cria Dicion\u00e1rio, modelo e matriz de similaridades Estes arquivos ser\u00e3o utilizados pelo ChatBot, tal qual os arquivos gerados anteriormente, para os Servi\u00e7os e para as Conversas Comuns. # Transforma a descricao das perguntas em tokens de palavras input_tokens = [d.split() for d in dataset1['desc'].values] # Criando um dicionario com os textos do dataset dct3 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus3 = [dct3.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model3 = TfidfModel(corpus3) # # Cria a matriz de similaridade e salva em um arquivo \u00edndice index3 = similarities.SparseMatrixSimilarity(model3[corpus3], num_features=len(dct3)) # Salva a matriz para uso no pipeline principal index3.save('index3')","title":"Gera Perguntas e Respostas das Orienta\u00e7\u00f5es"},{"location":"gera_orientacoes/#processa-links-e-gera-textos-com-as-orientacoes-tributarias","text":"O arquivo 04gera_banco_orientacoes.ipynb a seguir , possui o c\u00f3digo python que processa os links das orienta\u00e7\u00f5es tribut\u00e1rias extra\u00eddos na etapa anterior e vai fazer o Web Scraping dos textos de cada um dos links e gerar o Banco de Dados com as perguntas e respostas de cada assunto e criar o modelo de dados que ser\u00e1 utilizado pelo ChatBot para responder \u00e0s d\u00favidas formuladas pelos usu\u00e1rios. # Rotina que l\u00ea os arquivos .xml com os links extra\u00eddos para cada assunto com Orienta\u00e7\u00f5es Tribut\u00e1rias # e os processa para gerar o banco de dados com as perguntas e respostas que ser\u00e3o utilizadas pelo ChatBot","title":"Processa Links e gera Textos com as orienta\u00e7\u00f5es tribut\u00e1rias"},{"location":"gera_orientacoes/#importanto-as-bibliotecas-necessarias","text":"#IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS import pandas as pd import openpyxl import requests import urllib from urllib.request import urlopen import time from bs4 import BeautifulSoup import os import re from string import punctuation from collections import defaultdict from heapq import nlargest import nltk nltk.download('stopwords') nltk.download('punkt') from nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from nltk.probability import FreqDist","title":"Importanto as bibliotecas necess\u00e1rias"},{"location":"gera_orientacoes/#processa-os-links-de-orientacao-tributaria-para-o-assunto-restituicao-ressarcimento-reembolso-e-compensacao","text":"# # processando os links de orientacao tribut\u00e1ria para o assunto restituicao-ressarcimento-reembolso-e-compensacao with open('1_perdcomp.xml', 'r') as f: #l\u00ea arquivo .xml data = f.read() Bs_data = BeautifulSoup(data, \"xml\") # transforma arquivo lido em objeto BeautifulSoup lnks = Bs_data.find_all('value') # encontra todos os links lista = [] #cria vari\u00e1vel \"lista\" para armazenar os links ix = 0 for i in lnks: #acrescenta \u00e0 lista somente os links do assunto correspondente if i.text[0:120] == 'https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/restituicao-ressarcimento-reembolso-e-compensacao': lista.append(i.text) # print(len(lista)) #","title":"Processa os links de orientacao tribut\u00e1ria para o assunto restituicao, ressarcimento, reembolso e compensacao"},{"location":"gera_orientacoes/#eliminando-os-links-duplicados","text":"# pr\u00e9-processamento dos links, para eliminar links duplicados lista1 = [] for i in lista: ind1 = i.find('#') if ind1 != -1: st2 = i[0:ind1] else: st2 = i lista1.append(st2) # # len(lista1) # lista2 = [] for i in lista1: if i[-1] == \"/\": st2 = i[0:len(i)-1] else: st2 = i lista2.append(st2) # # len(lista1) # lista3 = [] # lista final com os links for i in lista2: if i not in lista3: lista3.append(i) # # len(lista3)","title":"Eliminando os links duplicados"},{"location":"gera_orientacoes/#criando-um-dataframe-com-os-links-e-salvando-em-planilha","text":"# Cria um DataFrame com os links, em ordem alfab\u00e9tica # e salva em uma planilha excel dfx = pd.DataFrame({'Links': lista3}) # sorted_dfx =dfx.sort_values(by=['Links']) #Salvando para uma planilha sorted_dfx.to_excel('./1_Links_perdcomp.xlsx', index = False) # # carregando os assuntos para o DataFrame df1 = pd.read_excel('./1_Links_perdcomp.xlsx') df1","title":"Criando um DataFrame com os links e salvando em planilha"},{"location":"gera_orientacoes/#acessa-cada-link-para-raspar-o-titulo-e-o-texto-sobre-cada-assunto","text":"# acessando cada link para \"raspar\" o titulo e o texto sobre cada assunto links1 = df1['Links'] links2 = [] servicos2 = [] htmls = [] oques = [] for i in range(len(links1)): try: if i < 0: print(\"#\", i) continue except: pass html = urlopen(str(links1[i])) print(\"#\", i) bs = BeautifulSoup(html, 'html.parser') #class=\"documentByLine\" id=\"plone-document-byline\" eConteudo = bs.find('div', id='plone-document-byline') titulo = bs.find('h1', class_='documentFirstHeading') #eConteudo = bs.find('h1', class_='documentFirstHeading') if eConteudo: print(links1[i]) try: div_txt = bs.find('div', id='content-core') texto = div_txt texto1 = div_txt.text except: div_txt = titulo texto = div_txt texto1 = \"*\" #print(texto[0:40]) links2.append(links1[i]) servicos2.append(titulo.text) htmls.append(texto) oques.append(texto1) time.sleep(3) ##","title":"Acessa cada link para \"raspar\" o titulo e o texto sobre cada assunto"},{"location":"gera_orientacoes/#criando-um-dataframe-com-os-textos-e-salvando-para-uma-planilha","text":"# salvando os textos para um dataframe dfy = pd.DataFrame({'Link': links2, 'Assunto': servicos2, 'Html': htmls, 'Texto': oques}) # #Salvando para uma planilha dfy.to_excel('./1_Perguntas_perdcomp.xlsx', index = False)","title":"Criando um DataFrame com os textos e salvando para uma planilha"},{"location":"gera_orientacoes/#processa-os-textos-para","text":"remover as palavras desnecess\u00e1rias; selecionar as senten\u00e7as mais importantes; extrair as palavras-chave de cada texto # remove as STOPWORDS e processa o texto para selecionar as senten\u00e7as mais # importantes , para em seguida extrair as palavras chave # stopwords1 = set(stopwords.words('portuguese') + list(punctuation) + ['\u2013']) #stopwords = stopwords.words('portuguese') + list(punctuation) + ['\u2013'] plx = ['``',\"''\"] df1 = pd.read_excel('./1_Perguntas_perdcomp.xlsx') # chave_lst = [] for i in range(len(df1)): sentencas = sent_tokenize(str(df1.iloc[i][2])) texto = \"\" for j in sentencas: texto = texto + j + \" \" texto = texto.strip().lower() palavras = word_tokenize(texto) palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords1] palavras_sem_stopwords1 = [] for pal in palavras_sem_stopwords: if pal not in plx: palavras_sem_stopwords1.append(pal) palavras_sem_stopwords = palavras_sem_stopwords1 frequencia = FreqDist(palavras_sem_stopwords) sentencas_importantes = defaultdict(int) for k, sentenca in enumerate(sentencas): for palavra in word_tokenize(sentenca.lower()): if palavra in frequencia: sentencas_importantes[k] += frequencia[palavra] idx_sentencas_importantes = nlargest(5, sentencas_importantes, sentencas_importantes.get) str_chave = str(df1.iloc[i][1]+' ')*3+ (sentencas[0]+ ' ')*3 for y in sorted(idx_sentencas_importantes): #print(y,sentencas[y]) str_chave = str_chave + \" \" + sentencas[y] chave_lst.append(str_chave)","title":"Processa os textos para:"},{"location":"gera_orientacoes/#salva-os-dados-em-uma-planilha","text":"# salva os textos em uma planilha # df1['texto_chave'] = chave_lst df1.to_excel('./1_Perguntas_perdcompFinal.xlsx', index = False) # len(df1) Todo o c\u00f3digo anterior utilizado para gerar a planilha 1_Perguntas_perdcompFinal.xlsx ser\u00e1 repetido para cada um dos outros 7 assuntos cujos links foram salvos em planilha, gerando ao final mais 7 planilhas iguais a esta planilha. Como o c\u00f3digo \u00e9 id\u00eantico, n\u00e3o ser\u00e1 repetido nem comentado novamente. Neste ponto consolidamos manualmente utilizando o pr\u00f3prio Microsoft Excel, as seguintes planilhas: - 1_Perguntas_perdcompFinal.xlsx; 2_Perguntas_certidoesFinal.xlsx; 3_Perguntas_cobrancasFinal.xlsx; 4_Perguntas_declaracoesFinal.xlsx 5_Perguntas_pagamentosFinal.xlsx; 6_Perguntas_tributosFinal.xlsx; 7_Perguntas_cadastrosFinal.xlsx , e 8_Perguntas_julgamentoFinal.xlsx Com isso foi gravada uma planilha de nome 0_Perguntas_orientacoes_Final.xlsx , contendo todos os links que constam em cada uma destas 8 planilhas. Seguindo com o processamento:","title":"Salva os dados em uma planilha"},{"location":"gera_orientacoes/#importa-a-biblioteca-spacy","text":"# importa a biblioteca spacy e carrega o corpus do idioma Portugu\u00eas import spacy nlp = spacy.load(\"pt_core_news_md\")","title":"Importa a biblioteca spacy"},{"location":"gera_orientacoes/#le-a-planilha-consolidada-manualmente-com-todos-os-links","text":"# l\u00ea uma planilha consolidada com o conte\u00fado dos 8 assuntos gravados nas planilhas anteriores # A consolida\u00e7\u00e3o foi feita manualmente, utilizando-se o pr\u00f3prio Excel # df1 = pd.read_excel('./0_Perguntas_orientacoes_Final.xlsx')","title":"L\u00ea a planilha consolidada manualmente, com todos os links:"},{"location":"gera_orientacoes/#importa-a-biblioteca-nltk","text":"# importa a biblioteca nltk from nltk.stem import RSLPStemmer from nltk.tokenize import sent_tokenize, word_tokenize nltk.download('rslp') ste = nltk.stem.RSLPStemmer()","title":"Importa a biblioteca nltk"},{"location":"gera_orientacoes/#rotina-que-extrai-as-palavras-chaves-de-cada-perguntaresposta","text":"Observa\u00e7\u00e3o: muito similar (quase id\u00eantica) as rotinas que processaram os servi\u00e7os no arquivo \"02extrai_palavras_chave.ipynb\" # rotina que extrai as palavras chave de cada pergunta/resposta, muito similar (quase id\u00eantica) # as rotinas que processaram os servi\u00e7os no arquivo \"02extrai_palavras_chave.ipynb\" def troca_acentos(txt): transTable = txt.maketrans(\"\u00e1\u00e0\u00e3\u00e9\u00ea\u00ed\u00f3\u00f4\u00f5\u00fa\u00c1\u00c3\u00c3\u00c9\u00ca\u00cd\u00d3\u00d4\u00d5\u00da\u00e7\u00c7\", \"aaaeeiooouaaaeeioooucc\") txt = txt.translate(transTable) return str(txt) # # # fun\u00e7\u00e3o que tokeniza as palavras # def tokenizar(texto): # # car_nok = ['SPACE', 'DET','ADP', 'CCONJ','PUNCT','PRON','SCONJ'] txx = texto docxx = nlp(txx) txx_lemma = [(token.text,token.orth_, token.lemma_, token.pos_) for token in docxx] pal_xx = '' plvxx = \"\" for i in txx_lemma: if i[3] not in car_nok and len(i[0]) > 2: plvxx = plvxx + ste.stem(i[0]) + ' ' plvxx = plvxx.strip() plvxx = troca_acentos(plvxx) return plvxx # fim funcao tokenizar # # fun\u00e7\u00e3o para limpar o texto def limpa_texto(txt_a_limpar): txt_ok = txt_a_limpar.lower() txt_ok = txt_ok.strip() return txt_ok # # fim da funcao limpa_texto # # # cria planilha com as palavras chave dos servi\u00e7os print(\"***inicio tokeniza\u00e7\u00e3o***\") plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df1)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df1.iloc[j][3] txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df1['chaves_serv'] = plvx1 # # print(\"####fim tokeniza\u00e7\u00e3o####\") #","title":"Rotina que extrai as palavras chaves de cada pergunta/resposta"},{"location":"gera_orientacoes/#salva-os-textos-em-uma-planilha","text":"# salva para uma planilha excel df1.to_excel('./0_Perguntas_orientacoes_Final2.xlsx', index = False) # # l\u00ea as perguntas e respostas salvas na planilha e carrega para o DataFrame df1 = pd.read_excel('./0_Perguntas_orientacoes_Final2.xlsx') #","title":"Salva os textos em uma planilha"},{"location":"gera_orientacoes/#importando-a-bibliote-gensim","text":"# importa a biblioteca gensim #!pip install gensim import gensim import gensim.downloader as api from gensim.models import TfidfModel from gensim.corpora import Dictionary from gensim import similarities","title":"Importando a bibliote gensim"},{"location":"gera_orientacoes/#cria-um-dataframe-com-as-palavras-chave","text":"# cria uma lista com as palavras-chave de cada pergunta/resposta servicos = [] for i in range(len(df1)): servicos.append(str(df1.iloc[i][5])) ids = [i for i in range(len(df1))] # # cria um DataFrame com a descri\u00e7\u00e3o e o id de cada pergunta dataset1 = pd.DataFrame({'id': ids,'desc': servicos})","title":"Cria um DataFrame com as palavras-chave"},{"location":"gera_orientacoes/#cria-dicionario-modelo-e-matriz-de-similaridades","text":"Estes arquivos ser\u00e3o utilizados pelo ChatBot, tal qual os arquivos gerados anteriormente, para os Servi\u00e7os e para as Conversas Comuns. # Transforma a descricao das perguntas em tokens de palavras input_tokens = [d.split() for d in dataset1['desc'].values] # Criando um dicionario com os textos do dataset dct3 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus3 = [dct3.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model3 = TfidfModel(corpus3) # # Cria a matriz de similaridade e salva em um arquivo \u00edndice index3 = similarities.SparseMatrixSimilarity(model3[corpus3], num_features=len(dct3)) # Salva a matriz para uso no pipeline principal index3.save('index3')","title":"Cria Dicion\u00e1rio, modelo e matriz de similaridades"},{"location":"implementacao/","text":"Implementa\u00e7\u00e3o Extra\u00e7\u00e3o/Raspagem dos Dados A coleta dos dados foi realizada atrav\u00e9s de c\u00f3digo desenvolvido em Python e executados no Jupyter Notebook, em um notebook de nome 01raspa_servicos.ypnb. Abaixo mostraremos o c\u00f3digo com coment\u00e1rios explicando a l\u00f3gica. Importa\u00e7\u00e3o das bibliotecas Python necess\u00e1rias #IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS from bs4 import BeautifulSoup import time import requests import urllib # importanto a biblioteca pandas import pandas as pd from urllib.request import urlopen # A biblioteca BeautifulSoup \u00e9 que foi utilizada para fazer o Web Scraping(raspagem dos dados). Gerando uma lista com as p\u00e1ginas de onde ser\u00e3o \"raspados\" os links dos servi\u00e7os # #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da RFB no .gov base1 = 'https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil?b_start:int=' #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da PGFN no .gov base2 = 'https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional?b_start:int=' # gerando lista com as p\u00e1ginas de onde ser\u00e3o extra\u00eddos os links para cada servi\u00e7o existente para os dois \u00f3rg\u00e3os, RFB e PGFN lista_pgs = [] y = 0 for i in range(6): # seis p\u00e1ginas com links de servi\u00e7os da RFB if i > 0: y = y + 30 lista_pgs.append(base1 + str(y)) y = 0 for i in range(2): # duas p\u00e1ginas com links de servi\u00e7os da PGFN if i > 0: y = y + 30 lista_pgs.append(base2 + str(y)) # Fazendo o scraping das informa\u00e7\u00f5es sobre cada servi\u00e7o # para cada link na lista dos servi\u00e7os, faz o Scraping (raspagem) dos t\u00edtulos e links de cada servi\u00e7o l_lnk = [] # lista para armazenar os links de cada servi\u00e7o l_titulo = [] # lista para armazenas o titulo do servi\u00e7o # for i in lista_pgs: # loop que itera sobre cada p\u00e1gina de onde ser\u00e3o extra\u00eddos os links dos servi\u00e7os try: html = urlopen(i) # acessa a p\u00e1gina que contem os links bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup com o conte\u00fado da p\u00e1gina pg = bs.find('ul', class_='listagem') # encontra a 'tag' <ul> , com classe listagem, que contem os links dos servi\u00e7os itens = pg.find_all('li', class_='item') # encontra a 'tag ' <li> , com classe item, onde est\u00e1 cada link ('tag' <a>) for j in itens: # loop sobre cada item, para extrair o t\u00edtulo, o link e a categoria de cada servi\u00e7o ln1 = j.find('a') # 'tag' <a> que contem o link de cada servi\u00e7o titx = ln1['title'] # atributo 't\u00edtulo' do servi\u00e7o lnx = ln1['href'] # atributo 'href' do servi\u00e7o (link) l_titulo.append(titx) # acrescenta o t\u00edtulo do servi\u00e7o a lista l_lnk.append(lnx) # acrescenta o link para o servi\u00e7o a lista except: #se houver erro n\u00e3o interrompe o programa pass Utilizando a biblioteca Pandas para gravar os dados \"raspados\" em uma planilha excel # grava uma planilha excel com os titulos e os links para cada servi\u00e7o, que ser\u00e3o utilizados para acessar # cada p\u00e1gina do servi\u00e7o e fazer a \"raspagem\" da descri\u00e7\u00e3o do servi\u00e7o dfx = pd.DataFrame({'Servico': l_titulo ,'Link': l_lnk}) dfx.to_excel('./ServicosRFBePGFN.xlsx', index = False) Acessa cada link gravado anteriormente para \"raspar\" o texto que descreve cada servi\u00e7o # c\u00f3digo que ir\u00e1 acessar cada link do servi\u00e7o para fazer a \"raspagem\" da texto que descreve o servi\u00e7o links = dfx['Link'] # lista com os links oques = [] # lista onde ser\u00e1 armazenada a descri\u00e7\u00e3o e cada um dos servi\u00e7os for i in range(len(links)): # itera sobre cada link e acessa a p\u00e1gina do servi\u00e7o try: if i < 0: # teste para fazer a raspagem a partir de um item da lista, com 'zero' raspa todos os itens print(\"#\", i) continue html = urlopen(str(links[i])) # acessa a p\u00e1gina do servi\u00e7o print(\"#\", i) # imprime o nr. do item e o link, apenas para visualizar como est\u00e1 o processamento except: continue bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup da p\u00e1gina print(links[i]) div_que = bs.find('div', class_='conteudo') # procura a 'tag' <div> onde est\u00e1 a descri\u00e7\u00e3o do servi\u00e7o texto = div_que.text # texto com a descri\u00e7\u00e3o de cada servi\u00e7o oques.append(texto) # adiciona a descri\u00e7\u00e3o do servi\u00e7o a lista time.sleep(3) # tempo de espera para n\u00e3o sobrecarregar o servidor .gov ## Acrescenta a planilha gravada o atributo com o texto que descreve o servi\u00e7o # # acrescenta a planilha criada anteriormente o atributo que descreve o servi\u00e7o dfx['oQue'] = oques # #Salvando a planilha # dfx.to_excel('./ServicosRFBePGFN_Final.xlsx', index = False) # Extra\u00e7\u00e3o das palavras chave dos servi\u00e7os Nome do notebook jupyter: 02extrai_palavras_chave.ypnb : Extraindo as senten\u00e7as principais do texto oQueE dos servi\u00e7os RFB Importando as bibliotecas #IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS import nltk import time import pandas as pd import os import re from nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from string import punctuation from nltk.probability import FreqDist from collections import defaultdict from heapq import nlargest # Extra\u00e7\u00e3o das palavras chaves das informa\u00e7\u00f5es de cada servi\u00e7o # c\u00f3digo para verificar as senten\u00e7as mais importantes no texto da descri\u00e7\u00e3o do servi\u00e7o, texto do qual # ser\u00e3o extra\u00eddas as palavras chave para utiliza\u00e7\u00e3o nos modelos utilizados pelo chatbot # # lista com as stopwords, ou seja, palavras n\u00e3o importantes e que podem ser descartadas stopwords1 = stopwords.words('portuguese') + list(punctuation) + ['\u2013'] plx = ['``',\"''\"] # caracteres de pontua\u00e7\u00e3o tamb\u00e9m indesej\u00e1veis # l\u00ea a planilha gerada anteriormente, com os dados dos servi\u00e7os df1 = pd.read_excel('./ServicosRFBePGFN_Final.xlsx') # chave_lst = [] # lista para guardar as palavras chave for i in range(len(df1)): # loop que itera sobre cada servi\u00e7o para extrair as frases mais importantes do texto sentencas = sent_tokenize(df1.iloc[i][2]) # separando o texto em senten\u00e7as texto = \"\" for j in sentencas: # converte a lista com as senten\u00e7as em string texto = texto + j + \" \" texto = texto.strip().lower() # transforma o texto em min\u00fasculas palavras = word_tokenize(texto) # tokeniza o texto em palavras palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords1] # retira as stopwords do texto palavras_sem_stopwords1 = [] for pal in palavras_sem_stopwords: #retira os caracteres indesej\u00e1veis do texto if pal not in plx: palavras_sem_stopwords1.append(pal) palavras_sem_stopwords = palavras_sem_stopwords1 frequencia = FreqDist(palavras_sem_stopwords) # cria a distribui\u00e7\u00e3o de frequ\u00eancia das palavras sentencas_importantes = defaultdict(int) # cria dicion\u00e1rio que conter\u00e1 as senten\u00e7as mais importantes for k, sentenca in enumerate(sentencas): #calcula a pontua\u00e7\u00e3o de cadas senten\u00e7a, baseado na frequ\u00eancia das palavras for palavra in word_tokenize(sentenca.lower()): if palavra in frequencia: sentencas_importantes[k] += frequencia[palavra] # separa as 3 senten\u00e7as com maior pontua\u00e7\u00e3o idx_sentencas_importantes = nlargest(3, sentencas_importantes, sentencas_importantes.get) #atribui peso 4 ao t\u00edtulo do servi\u00e7o e peso 3 para a senten\u00e7a mais importante do texto da descri\u00e7\u00e3o do servi\u00e7o str_chave = str(df1.iloc[i][0]+' ')*4 + (sentencas[0]+ ' ')*3 # string com as palavras chave for y in sorted(idx_sentencas_importantes): # acrescenta as senten\u00e7as importantes na string de palavras chave str_chave = str_chave + \" \" + sentencas[y] chave_lst.append(str_chave) # acrescenta a string de palavras chave a lista # # para compor a lista das palavras chaves, foi atribuido peso 4 para o t\u00edtulo do servi\u00e7o, peso 4 para a # frase mais importante e peso 1 para as outras duas frases importantes # Grava os servi\u00e7os em uma planilha df1['texto_chave'] = chave_lst df1.to_excel('./ServicosRfbFinal.xlsx', index = False) # len(df1) # n\u00famero de servi\u00e7os da base de dados = 181 Carrega os dataframes com o conte\u00fado das duas planilhas df1 = pd.read_excel('./ServicosRfbFinal.xlsx') df2 = pd.read_excel('./PerguntasTreinamento.xlsx') # planilha criada manualmente com perguntas/respostas comuns # carrega bibliotecas de NLP, spacy e nltk import spacy nlp = spacy.load(\"pt_core_news_md\") #carrega o corpus em portugu\u00eas do spacy from nltk.stem import RSLPStemmer from nltk.tokenize import sent_tokenize, word_tokenize nltk.download('rslp') ste = nltk.stem.RSLPStemmer() Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chave dos servi\u00e7os def troca_acentos(txt): #fun\u00e7\u00e3o que retira os acentos da l\u00edngua transTable = txt.maketrans(\"\u00e1\u00e0\u00e3\u00e9\u00ea\u00ed\u00f3\u00f4\u00f5\u00fa\u00c1\u00c3\u00c3\u00c9\u00ca\u00cd\u00d3\u00d4\u00d5\u00da\u00e7\u00c7\", \"aaaeeiooouaaaeeioooucc\") txt = txt.translate(transTable) return str(txt) # # # fun\u00e7\u00e3o que tokeniza as palavras # def tokenizar(texto): # fun\u00e7\u00e3o que tokeniza o texto, e deixa somente palavras significativas # car_nok = ['SPACE', 'DET','ADP', 'CCONJ','PUNCT','PRON','SCONJ'] txx = texto docxx = nlp(txx) txx_lemma = [(token.text,token.orth_, token.lemma_, token.pos_) for token in docxx] #reduz a palavra ao seu Lemma pal_xx = '' plvxx = \"\" for i in txx_lemma: if i[3] not in car_nok and len(i[0]) > 2: plvxx = plvxx + ste.stem(i[0]) + ' ' plvxx = plvxx.strip() plvxx = troca_acentos(plvxx) return plvxx # fim funcao tokenizar # # fun\u00e7\u00e3o para limpar o texto def limpa_texto(txt_a_limpar): # transforma em min\u00fasculas e retira espa;os iniciais e finais txt_ok = txt_a_limpar.lower() txt_ok = txt_ok.strip() return txt_ok # # fim da funcao limpa_texto # # # cria planilha com as palavras chave dos servi\u00e7os print(\"***inicio tokeniza\u00e7\u00e3o dos servi\u00e7os***\") plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df1)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df1.iloc[j][3] txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df1['chaves_serv'] = plvx1 # # print(\"#### fim tokeniza\u00e7\u00e3o dos servi\u00e7os ####\") # Grava vers\u00e3o final da planilha de servi\u00e7os # grava as palavras chave na planilha de servi\u00e7os df1.to_excel('./ServicosRfbFinal2.xlsx', index = False) Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chaves das frases de conversas comuns print(\"*** inicio tokeniza\u00e7\u00e3o das conversas ***\") # cria planilha com as palavras chave dos servi\u00e7os plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df2)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df2.iloc[j][0] + ' ' txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df2['chaves_serv'] = plvx1 # print(\"#### fim tokeniza\u00e7\u00e3o das conversas ####\") Criando os modelos NLP utilizados pelo Bot Continua\u00e7\u00e3o do notebook jupyter: 02extrai_palavras_chave.ypnb : importando os pacotes gensim #!pip install gensim import gensim import gensim.downloader as api from gensim.models import TfidfModel from gensim.corpora import Dictionary from gensim import similarities Lendo dados para treinamento (servi\u00e7os e conversas comuns) # df1 = pd.read_excel('./ServicosRfbFinal2.xlsx') # servicos = [] for i in range(len(df1)): servicos.append(str(df1.iloc[i][4])) ids = [i for i in range(len(df1))] dataset1 = pd.DataFrame({'id': ids,'desc': servicos}) # df2 = pd.read_excel('./PerguntasTreinamento2.xlsx') # conversas = [] for i in range(len(df2)): conversas.append(str(df2.iloc[i][3])) ids = [i for i in range(len(df2))] dataset2 = pd.DataFrame({'id': ids,'desc': conversas}) Modelo Bag of Words(Bow) - Criando um dicion\u00e1rio, um corpus e um modelo # Transforma a descricao do servico em tokens de palavras input_tokens = [d.split() for d in dataset1['desc'].values] # Criando um dicionario com os textos do dataset dct1 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus1 = [dct1.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model1 = TfidfModel(corpus1) # # Transforma as conversas de treinamento em tokens de palavras input_tokens = [d.split() for d in dataset2['desc'].values] # Criando um dicionario com os textos do dataset dct2 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus2 = [dct2.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model2 = TfidfModel(corpus2) # # Salvando os modelos e os dicion\u00e1rios para uso do Chatbot # dct1.save('dct1') model1.save('model1') dct2.save('dct2') model2.save('model2') OBSERVA\u00c7\u00d5ES componentes do pipeline para criar o modelo (c\u00f3digo acima) input_tokens -> separa as palavras de cada texto; dct -> lista cada palavra dos textos em um dicionario; corpus -> faz um mapeamento de cada texto com os ids do dicionario (id da palavra, quantidade de ocorrencias); model -> \u00e9 o transformer responsavel por traduzir o corpus em um TF-IDF Criando uma matriz de similaridades At\u00e9 aqui fizemos um pipeline que transformou textos em matrizes TF-IDF e agora podemos comparar duas TF-IDFs e verificar sua similaridade. Ent\u00e3o iremos criar duas matrizes de similaridade com todos os textos dos Servi\u00e7os e das conversar comuns, que chamaremos de index1 e index2. Com as matrizes criadas poderemos comparar o texto digitado pelo usu\u00e1rio com os textos existentes em nossa base de dados. # Cria a matriz de similaridade dos Servi\u00e7os index1 = similarities.SparseMatrixSimilarity(model1[corpus1], num_features=len(dct1)) # Salva a matriz para uso no pipeline principal index1.save('index1') # # Cria a matriz de similaridade das conversas comuns index2 = similarities.SparseMatrixSimilarity(model2[corpus2], num_features=len(dct2)) # Salva a matriz para uso no pipeline principal index2.save('index2')","title":"Implementacao"},{"location":"implementacao/#implementacao","text":"","title":"Implementa\u00e7\u00e3o"},{"location":"implementacao/#extracaoraspagem-dos-dados","text":"A coleta dos dados foi realizada atrav\u00e9s de c\u00f3digo desenvolvido em Python e executados no Jupyter Notebook, em um notebook de nome 01raspa_servicos.ypnb. Abaixo mostraremos o c\u00f3digo com coment\u00e1rios explicando a l\u00f3gica.","title":"Extra\u00e7\u00e3o/Raspagem dos Dados"},{"location":"implementacao/#importacao-das-bibliotecas-python-necessarias","text":"#IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS from bs4 import BeautifulSoup import time import requests import urllib # importanto a biblioteca pandas import pandas as pd from urllib.request import urlopen # A biblioteca BeautifulSoup \u00e9 que foi utilizada para fazer o Web Scraping(raspagem dos dados).","title":"Importa\u00e7\u00e3o das bibliotecas Python necess\u00e1rias"},{"location":"implementacao/#gerando-uma-lista-com-as-paginas-de-onde-serao-raspados-os-links-dos-servicos","text":"# #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da RFB no .gov base1 = 'https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil?b_start:int=' #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da PGFN no .gov base2 = 'https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional?b_start:int=' # gerando lista com as p\u00e1ginas de onde ser\u00e3o extra\u00eddos os links para cada servi\u00e7o existente para os dois \u00f3rg\u00e3os, RFB e PGFN lista_pgs = [] y = 0 for i in range(6): # seis p\u00e1ginas com links de servi\u00e7os da RFB if i > 0: y = y + 30 lista_pgs.append(base1 + str(y)) y = 0 for i in range(2): # duas p\u00e1ginas com links de servi\u00e7os da PGFN if i > 0: y = y + 30 lista_pgs.append(base2 + str(y)) #","title":"Gerando uma lista com as p\u00e1ginas de onde ser\u00e3o \"raspados\" os links dos servi\u00e7os"},{"location":"implementacao/#fazendo-o-scraping-das-informacoes-sobre-cada-servico","text":"# para cada link na lista dos servi\u00e7os, faz o Scraping (raspagem) dos t\u00edtulos e links de cada servi\u00e7o l_lnk = [] # lista para armazenar os links de cada servi\u00e7o l_titulo = [] # lista para armazenas o titulo do servi\u00e7o # for i in lista_pgs: # loop que itera sobre cada p\u00e1gina de onde ser\u00e3o extra\u00eddos os links dos servi\u00e7os try: html = urlopen(i) # acessa a p\u00e1gina que contem os links bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup com o conte\u00fado da p\u00e1gina pg = bs.find('ul', class_='listagem') # encontra a 'tag' <ul> , com classe listagem, que contem os links dos servi\u00e7os itens = pg.find_all('li', class_='item') # encontra a 'tag ' <li> , com classe item, onde est\u00e1 cada link ('tag' <a>) for j in itens: # loop sobre cada item, para extrair o t\u00edtulo, o link e a categoria de cada servi\u00e7o ln1 = j.find('a') # 'tag' <a> que contem o link de cada servi\u00e7o titx = ln1['title'] # atributo 't\u00edtulo' do servi\u00e7o lnx = ln1['href'] # atributo 'href' do servi\u00e7o (link) l_titulo.append(titx) # acrescenta o t\u00edtulo do servi\u00e7o a lista l_lnk.append(lnx) # acrescenta o link para o servi\u00e7o a lista except: #se houver erro n\u00e3o interrompe o programa pass","title":"Fazendo o scraping das informa\u00e7\u00f5es sobre cada servi\u00e7o"},{"location":"implementacao/#utilizando-a-biblioteca-pandas-para-gravar-os-dados-raspados-em-uma-planilha-excel","text":"# grava uma planilha excel com os titulos e os links para cada servi\u00e7o, que ser\u00e3o utilizados para acessar # cada p\u00e1gina do servi\u00e7o e fazer a \"raspagem\" da descri\u00e7\u00e3o do servi\u00e7o dfx = pd.DataFrame({'Servico': l_titulo ,'Link': l_lnk}) dfx.to_excel('./ServicosRFBePGFN.xlsx', index = False)","title":"Utilizando a biblioteca Pandas para gravar os dados \"raspados\" em uma planilha excel"},{"location":"implementacao/#acessa-cada-link-gravado-anteriormente-para-raspar-o-texto-que-descreve-cada-servico","text":"# c\u00f3digo que ir\u00e1 acessar cada link do servi\u00e7o para fazer a \"raspagem\" da texto que descreve o servi\u00e7o links = dfx['Link'] # lista com os links oques = [] # lista onde ser\u00e1 armazenada a descri\u00e7\u00e3o e cada um dos servi\u00e7os for i in range(len(links)): # itera sobre cada link e acessa a p\u00e1gina do servi\u00e7o try: if i < 0: # teste para fazer a raspagem a partir de um item da lista, com 'zero' raspa todos os itens print(\"#\", i) continue html = urlopen(str(links[i])) # acessa a p\u00e1gina do servi\u00e7o print(\"#\", i) # imprime o nr. do item e o link, apenas para visualizar como est\u00e1 o processamento except: continue bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup da p\u00e1gina print(links[i]) div_que = bs.find('div', class_='conteudo') # procura a 'tag' <div> onde est\u00e1 a descri\u00e7\u00e3o do servi\u00e7o texto = div_que.text # texto com a descri\u00e7\u00e3o de cada servi\u00e7o oques.append(texto) # adiciona a descri\u00e7\u00e3o do servi\u00e7o a lista time.sleep(3) # tempo de espera para n\u00e3o sobrecarregar o servidor .gov ##","title":"Acessa cada link gravado anteriormente para \"raspar\" o texto que descreve cada servi\u00e7o"},{"location":"implementacao/#acrescenta-a-planilha-gravada-o-atributo-com-o-texto-que-descreve-o-servico","text":"# # acrescenta a planilha criada anteriormente o atributo que descreve o servi\u00e7o dfx['oQue'] = oques # #Salvando a planilha # dfx.to_excel('./ServicosRFBePGFN_Final.xlsx', index = False) #","title":"Acrescenta a planilha gravada o atributo com o texto que descreve o servi\u00e7o"},{"location":"implementacao/#extracao-das-palavras-chave-dos-servicos","text":"Nome do notebook jupyter: 02extrai_palavras_chave.ypnb :","title":"Extra\u00e7\u00e3o das palavras chave dos servi\u00e7os"},{"location":"implementacao/#extraindo-as-sentencas-principais-do-texto-oquee-dos-servicos-rfb","text":"","title":"Extraindo as senten\u00e7as principais do texto oQueE dos servi\u00e7os RFB"},{"location":"implementacao/#importando-as-bibliotecas","text":"#IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS import nltk import time import pandas as pd import os import re from nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from string import punctuation from nltk.probability import FreqDist from collections import defaultdict from heapq import nlargest #","title":"Importando as bibliotecas"},{"location":"implementacao/#extracao-das-palavras-chaves-das-informacoes-de-cada-servico","text":"# c\u00f3digo para verificar as senten\u00e7as mais importantes no texto da descri\u00e7\u00e3o do servi\u00e7o, texto do qual # ser\u00e3o extra\u00eddas as palavras chave para utiliza\u00e7\u00e3o nos modelos utilizados pelo chatbot # # lista com as stopwords, ou seja, palavras n\u00e3o importantes e que podem ser descartadas stopwords1 = stopwords.words('portuguese') + list(punctuation) + ['\u2013'] plx = ['``',\"''\"] # caracteres de pontua\u00e7\u00e3o tamb\u00e9m indesej\u00e1veis # l\u00ea a planilha gerada anteriormente, com os dados dos servi\u00e7os df1 = pd.read_excel('./ServicosRFBePGFN_Final.xlsx') # chave_lst = [] # lista para guardar as palavras chave for i in range(len(df1)): # loop que itera sobre cada servi\u00e7o para extrair as frases mais importantes do texto sentencas = sent_tokenize(df1.iloc[i][2]) # separando o texto em senten\u00e7as texto = \"\" for j in sentencas: # converte a lista com as senten\u00e7as em string texto = texto + j + \" \" texto = texto.strip().lower() # transforma o texto em min\u00fasculas palavras = word_tokenize(texto) # tokeniza o texto em palavras palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords1] # retira as stopwords do texto palavras_sem_stopwords1 = [] for pal in palavras_sem_stopwords: #retira os caracteres indesej\u00e1veis do texto if pal not in plx: palavras_sem_stopwords1.append(pal) palavras_sem_stopwords = palavras_sem_stopwords1 frequencia = FreqDist(palavras_sem_stopwords) # cria a distribui\u00e7\u00e3o de frequ\u00eancia das palavras sentencas_importantes = defaultdict(int) # cria dicion\u00e1rio que conter\u00e1 as senten\u00e7as mais importantes for k, sentenca in enumerate(sentencas): #calcula a pontua\u00e7\u00e3o de cadas senten\u00e7a, baseado na frequ\u00eancia das palavras for palavra in word_tokenize(sentenca.lower()): if palavra in frequencia: sentencas_importantes[k] += frequencia[palavra] # separa as 3 senten\u00e7as com maior pontua\u00e7\u00e3o idx_sentencas_importantes = nlargest(3, sentencas_importantes, sentencas_importantes.get) #atribui peso 4 ao t\u00edtulo do servi\u00e7o e peso 3 para a senten\u00e7a mais importante do texto da descri\u00e7\u00e3o do servi\u00e7o str_chave = str(df1.iloc[i][0]+' ')*4 + (sentencas[0]+ ' ')*3 # string com as palavras chave for y in sorted(idx_sentencas_importantes): # acrescenta as senten\u00e7as importantes na string de palavras chave str_chave = str_chave + \" \" + sentencas[y] chave_lst.append(str_chave) # acrescenta a string de palavras chave a lista # # para compor a lista das palavras chaves, foi atribuido peso 4 para o t\u00edtulo do servi\u00e7o, peso 4 para a # frase mais importante e peso 1 para as outras duas frases importantes #","title":"Extra\u00e7\u00e3o das palavras chaves das informa\u00e7\u00f5es de cada servi\u00e7o"},{"location":"implementacao/#grava-os-servicos-em-uma-planilha","text":"df1['texto_chave'] = chave_lst df1.to_excel('./ServicosRfbFinal.xlsx', index = False) # len(df1) # n\u00famero de servi\u00e7os da base de dados = 181","title":"Grava os servi\u00e7os em uma planilha"},{"location":"implementacao/#carrega-os-dataframes-com-o-conteudo-das-duas-planilhas","text":"df1 = pd.read_excel('./ServicosRfbFinal.xlsx') df2 = pd.read_excel('./PerguntasTreinamento.xlsx') # planilha criada manualmente com perguntas/respostas comuns #","title":"Carrega os dataframes com o conte\u00fado das duas planilhas"},{"location":"implementacao/#carrega-bibliotecas-de-nlp-spacy-e-nltk","text":"import spacy nlp = spacy.load(\"pt_core_news_md\") #carrega o corpus em portugu\u00eas do spacy from nltk.stem import RSLPStemmer from nltk.tokenize import sent_tokenize, word_tokenize nltk.download('rslp') ste = nltk.stem.RSLPStemmer()","title":"carrega bibliotecas de NLP, spacy e nltk"},{"location":"implementacao/#tokenizacao-e-extracao-das-palavras-chave-dos-servicos","text":"def troca_acentos(txt): #fun\u00e7\u00e3o que retira os acentos da l\u00edngua transTable = txt.maketrans(\"\u00e1\u00e0\u00e3\u00e9\u00ea\u00ed\u00f3\u00f4\u00f5\u00fa\u00c1\u00c3\u00c3\u00c9\u00ca\u00cd\u00d3\u00d4\u00d5\u00da\u00e7\u00c7\", \"aaaeeiooouaaaeeioooucc\") txt = txt.translate(transTable) return str(txt) # # # fun\u00e7\u00e3o que tokeniza as palavras # def tokenizar(texto): # fun\u00e7\u00e3o que tokeniza o texto, e deixa somente palavras significativas # car_nok = ['SPACE', 'DET','ADP', 'CCONJ','PUNCT','PRON','SCONJ'] txx = texto docxx = nlp(txx) txx_lemma = [(token.text,token.orth_, token.lemma_, token.pos_) for token in docxx] #reduz a palavra ao seu Lemma pal_xx = '' plvxx = \"\" for i in txx_lemma: if i[3] not in car_nok and len(i[0]) > 2: plvxx = plvxx + ste.stem(i[0]) + ' ' plvxx = plvxx.strip() plvxx = troca_acentos(plvxx) return plvxx # fim funcao tokenizar # # fun\u00e7\u00e3o para limpar o texto def limpa_texto(txt_a_limpar): # transforma em min\u00fasculas e retira espa;os iniciais e finais txt_ok = txt_a_limpar.lower() txt_ok = txt_ok.strip() return txt_ok # # fim da funcao limpa_texto # # # cria planilha com as palavras chave dos servi\u00e7os print(\"***inicio tokeniza\u00e7\u00e3o dos servi\u00e7os***\") plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df1)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df1.iloc[j][3] txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df1['chaves_serv'] = plvx1 # # print(\"#### fim tokeniza\u00e7\u00e3o dos servi\u00e7os ####\") #","title":"Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chave dos servi\u00e7os"},{"location":"implementacao/#grava-versao-final-da-planilha-de-servicos","text":"# grava as palavras chave na planilha de servi\u00e7os df1.to_excel('./ServicosRfbFinal2.xlsx', index = False)","title":"Grava vers\u00e3o final da planilha de servi\u00e7os"},{"location":"implementacao/#tokenizacao-e-extracao-das-palavras-chaves-das-frases-de-conversas-comuns","text":"print(\"*** inicio tokeniza\u00e7\u00e3o das conversas ***\") # cria planilha com as palavras chave dos servi\u00e7os plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df2)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df2.iloc[j][0] + ' ' txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df2['chaves_serv'] = plvx1 # print(\"#### fim tokeniza\u00e7\u00e3o das conversas ####\")","title":"Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chaves das frases de conversas comuns"},{"location":"implementacao/#criando-os-modelos-nlp-utilizados-pelo-bot","text":"Continua\u00e7\u00e3o do notebook jupyter: 02extrai_palavras_chave.ypnb :","title":"Criando os modelos NLP utilizados pelo Bot"},{"location":"implementacao/#importando-os-pacotes-gensim","text":"#!pip install gensim import gensim import gensim.downloader as api from gensim.models import TfidfModel from gensim.corpora import Dictionary from gensim import similarities","title":"importando os pacotes gensim"},{"location":"implementacao/#lendo-dados-para-treinamento-servicos-e-conversas-comuns","text":"# df1 = pd.read_excel('./ServicosRfbFinal2.xlsx') # servicos = [] for i in range(len(df1)): servicos.append(str(df1.iloc[i][4])) ids = [i for i in range(len(df1))] dataset1 = pd.DataFrame({'id': ids,'desc': servicos}) # df2 = pd.read_excel('./PerguntasTreinamento2.xlsx') # conversas = [] for i in range(len(df2)): conversas.append(str(df2.iloc[i][3])) ids = [i for i in range(len(df2))] dataset2 = pd.DataFrame({'id': ids,'desc': conversas})","title":"Lendo dados para treinamento (servi\u00e7os e conversas comuns)"},{"location":"implementacao/#modelo-bag-of-wordsbow-criando-um-dicionario-um-corpus-e-um-modelo","text":"# Transforma a descricao do servico em tokens de palavras input_tokens = [d.split() for d in dataset1['desc'].values] # Criando um dicionario com os textos do dataset dct1 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus1 = [dct1.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model1 = TfidfModel(corpus1) # # Transforma as conversas de treinamento em tokens de palavras input_tokens = [d.split() for d in dataset2['desc'].values] # Criando um dicionario com os textos do dataset dct2 = Dictionary(input_tokens) # Converte corpus para o formato BOW (Bag Of Words) corpus2 = [dct2.doc2bow(line) for line in input_tokens] # Cria um modelo TF-IDF model2 = TfidfModel(corpus2) # # Salvando os modelos e os dicion\u00e1rios para uso do Chatbot # dct1.save('dct1') model1.save('model1') dct2.save('dct2') model2.save('model2') OBSERVA\u00c7\u00d5ES","title":"Modelo Bag of Words(Bow) - Criando um dicion\u00e1rio, um corpus e um modelo"},{"location":"implementacao/#componentes-do-pipeline-para-criar-o-modelo-codigo-acima","text":"input_tokens -> separa as palavras de cada texto; dct -> lista cada palavra dos textos em um dicionario; corpus -> faz um mapeamento de cada texto com os ids do dicionario (id da palavra, quantidade de ocorrencias); model -> \u00e9 o transformer responsavel por traduzir o corpus em um TF-IDF","title":"componentes do pipeline para criar o modelo (c\u00f3digo acima)"},{"location":"implementacao/#criando-uma-matriz-de-similaridades","text":"At\u00e9 aqui fizemos um pipeline que transformou textos em matrizes TF-IDF e agora podemos comparar duas TF-IDFs e verificar sua similaridade. Ent\u00e3o iremos criar duas matrizes de similaridade com todos os textos dos Servi\u00e7os e das conversar comuns, que chamaremos de index1 e index2. Com as matrizes criadas poderemos comparar o texto digitado pelo usu\u00e1rio com os textos existentes em nossa base de dados. # Cria a matriz de similaridade dos Servi\u00e7os index1 = similarities.SparseMatrixSimilarity(model1[corpus1], num_features=len(dct1)) # Salva a matriz para uso no pipeline principal index1.save('index1') # # Cria a matriz de similaridade das conversas comuns index2 = similarities.SparseMatrixSimilarity(model2[corpus2], num_features=len(dct2)) # Salva a matriz para uso no pipeline principal index2.save('index2')","title":"Criando uma matriz de similaridades"},{"location":"index_old/","text":"Desenvolvimento de um chatbot para orienta\u00e7\u00e3o do contribuinte que procura o atendimento da Receita Federal do Brasil (RFB) Autores Adriano S. Zumba, Carlo C. Taglialegna e Reynaldo Assun\u00e7\u00e3o RESUMO Este artigo se destina a propor o desenvolvimento de um assistente virtual (chatbot) que seja capaz de interagir com o contribuinte e direcion\u00e1-lo rapidamente ao servi\u00e7o demandado ou \u00e0s orienta\u00e7\u00f5es tribut\u00e1rias necess\u00e1rias, tendo em vista a grande quantidade de servi\u00e7os ofertados e os diversos canais de atendimento existentes na RFB. O objetivo do chatbot \u00e9 diminuir a dificuldade encontrada pelos contribuintes quando da demanda por um servi\u00e7o espec\u00edfico e evitar a navega\u00e7\u00e3o desnecess\u00e1ria pelas muitas p\u00e1ginas existentes no site da Receita Federal do Brasil. Palavras-chave: Chatbot; Atendimento; Receita Federal do Brasil. ABSTRACT This article is intended to propose the development of a virtual assistant (chatbot) that is able to interact with the taxpayer and quickly direct him to the required service or the necessary tax guidelines, aiming at the large amount of services offered and the various channels service. The purpose of the chatbot is to reduce the difficulty encountered by taxpayers when they demand a specific service and to avoid unnecessary navigation through the many pages on the Receita Federal do Brasil website. Key words: Chatbot; Atendimento; Receita Federal. Data de submiss\u00e3o: dia.m\u00eas.ano Data de aprova\u00e7\u00e3o: dia.m\u00eas.ano Disponibilidade: (endere\u00e7o eletr\u00f4nico do artigo, DOI ou outras informa\u00e7\u00f5es). 1. Introdu\u00e7\u00e3o A Receita Federal do Brasil administra um n\u00famero expressivo de tributos e contribui\u00e7\u00f5es, o que implica em uma quantidade enorme de assuntos e servi\u00e7os relacionados a eles. A orienta\u00e7\u00e3o ao contribuinte sobre esses temas, bem como a presta\u00e7\u00e3o de uma quantidade enorme de servi\u00e7os dispon\u00edveis, cabe a uma rede de atendimento composta por cerca de uma dezena de canais de atendimento existentes na estrutura do \u00f3rg\u00e3o. A porta de entrada para o contribuinte \u00e9 o site da institui\u00e7\u00e3o, que pode ser acessado pelo endere\u00e7o https://www.gov.br/receitafederal/pt-br , o qual, a despeito de ser bastante organizado, contendo informa\u00e7\u00f5es detalhadas sobre cada servi\u00e7o/assunto, exige do contribuinte a navega\u00e7\u00e3o por muitas p\u00e1ginas dispostas em uma grande \u00e1rvore de menus, o que dificulta muito a busca pelo atendimento de que necessita. A exist\u00eancia dessa diversidade de canais de atendimento torna dif\u00edcil a identifica\u00e7\u00e3o do canal de atendimento apropriado para a realiza\u00e7\u00e3o de um determinado servi\u00e7o ou consulta. Muitas vezes, o contribuinte tem que percorrer v\u00e1rios desses canais at\u00e9 encontrar a solu\u00e7\u00e3o para o seu problema, numa busca intrincada que leva a uma certa perda de tempo. Por outro lado, as regras que estabelecem a compet\u00eancia de cada um dos canais de atendimento t\u00eam sido modificadas com muita frequ\u00eancia desde o advento da pandemia do novo coronav\u00edrus, ocasionando, muitas vezes, conflitos de compet\u00eancia entre as atividades desses canais. Com isso, observa-se uma certa inefici\u00eancia e demora no atendimento das solicita\u00e7\u00f5es dos contribuintes, gerando uma grande insatisfa\u00e7\u00e3o por parte dos mesmos. Como consequ\u00eancia, temos um aumento no n\u00famero de reclama\u00e7\u00f5es nos diversos canais de Ouvidoria. Em \u00e9pocas de restri\u00e7\u00f5es \u00e0 presen\u00e7a do contribuinte em nossas unidades de atendimento, como a que estamos vivendo, o atendimento virtual se apresenta como uma alternativa vi\u00e1vel para manter a disponibilidade na presta\u00e7\u00e3o de servi\u00e7os aos contribuintes. Some-se a isso o fechamento de muitos postos de atendimento presenciais. Dessa forma, o uso dessas tecnologias possibilita n\u00e3o s\u00f3 a alta disponibilidade dos servi\u00e7os, mas tamb\u00e9m a redu\u00e7\u00e3o dos custos operacionais associados. Diante do cen\u00e1rio aqui exposto, propomos o desenvolvimento de um assistente virtual (chatbot) que seja capaz de interagir com o contribuinte, de modo que ele possa encontrar rapidamente o servi\u00e7o demandado ou as orienta\u00e7\u00f5es tribut\u00e1rias de que necessita, sem ter que navegar pelas muitas p\u00e1ginas existentes no site da Receita Federal. Em nosso modelo proposto, o chatbot indicar\u00e1 onde o usu\u00e1rio obter orienta\u00e7\u00f5es tribut\u00e1rias e/ou informa\u00e7\u00f5es sobre o Servi\u00e7o que procura, para ter a sua solicita\u00e7\u00e3o atendida com rapidez e efici\u00eancia. A introdu\u00e7\u00e3o \u00e9 o primeiro elemento textual. Nela deve-se expor a finalidade e os objetivos do trabalho de modo que o leitor tenha uma vis\u00e3o do tema abordado. De modo geral, a introdu\u00e7\u00e3o deve apresentar: a) as caracter\u00edsticas do conte\u00fado a ser explorado; b) o objeto do trabalho e sua delimita\u00e7\u00e3o (delimitar de forma clara os limites da pesquisa); c) o est\u00e1gio do desenvolvimento do assunto, isto \u00e9, apresentar o que existe publicado sobre o assunto; d) o problema; e) o(s) objetivo(s); f) quando necess\u00e1rio, hip\u00f3teses ou vari\u00e1veis; g) justificativa do estudo; h) metodologia utilizada; i) refer\u00eancia as partes do trabalho e as j) possibilidades de contribui\u00e7\u00e3o da pesquisa, sem anunciar conclus\u00f5es e solu\u00e7\u00f5es (MEDEIROS, 2007).* 2. Desenvolvimento Parte principal e mais extensa do trabalho deve apresentar a fundamenta\u00e7\u00e3o te\u00f3rica, a metodologia, os resultados e a discuss\u00e3o. Divide-se em se\u00e7\u00f5es e subse\u00e7\u00f5es conforme a NBR 6024 (ASSOCIA\u00c7\u00c3O BRASILEIRA DE NORMAS T\u00c9CNICAS, 2018a). 3. Considera\u00e7\u00f5es Finais Espera-se com esse projeto criar uma ferramenta que possua uma interface amig\u00e1vel e eficiente, que facilite ao cidad\u00e3o o atendimento de suas solicita\u00e7\u00f5es, fornecendo informa\u00e7\u00f5es de forma \u00e1gil e \u00fatil ao mesmo tempo, em conson\u00e2ncia com um dos pilares fundamentais dos princ\u00edpios que norteiam a administra\u00e7\u00e3o p\u00fablica: a efici\u00eancia. A democratiza\u00e7\u00e3o das informa\u00e7\u00f5es por meio de uma interface mais amig\u00e1vel \u00e9 fundamental para garantir a presta\u00e7\u00e3o de um atendimento de qualidade, reduzindo custos de trabalho adicionais por parte da administra\u00e7\u00e3o; propiciando padroniza\u00e7\u00e3o, conformidade e redu\u00e7\u00e3o de deslocamentos desnecess\u00e1rios do cidad\u00e3o; e uma maior disponibilidade de atendimento, que, no caso, seria em tempo integral. Portanto, o objetivo do presente trabalho \u00e9 demonstrar - sem, no entanto, esgotar o tema - que a partir de dados atuais disponibilizados em portais p\u00fablicos, como o da RFB, \u00e9 poss\u00edvel, com o uso de tecnologias e recursos de c\u00f3digo aberto, desenvolver um chatbot funcional que facilite a presta\u00e7\u00e3o eficiente de um servi\u00e7o de grande relev\u00e2ncia ao contribuinte, que \u00e9 uma orienta\u00e7\u00e3o tribut\u00e1ria adequada e informa\u00e7\u00f5es sobre como obter um determinado servi\u00e7o neste \u00d3rg\u00e3o, evitando assim perda de tempo e reclama\u00e7\u00f5es formais desnecess\u00e1rias. Parte final do artigo, onde se apresentam as conclus\u00f5es obtidas, mesmo que parciais. As conclus\u00f5es devem responder \u00e0s quest\u00f5es da pesquisa, correspondentes aos objetivos e hip\u00f3teses. Al\u00e9m disso, devem ser breves podendo apresentar recomenda\u00e7\u00f5es e sugest\u00f5es para trabalhos futuros. Refer\u00eancias Gloss\u00e1rio Ap\u00eandice Anexos Agradecimentos","title":"Index old"},{"location":"index_old/#desenvolvimento-de-um-chatbot-para-orientacao-do-contribuinte-que-procura-o-atendimento-da-receita-federal-do-brasil-rfb","text":"","title":"Desenvolvimento de um chatbot para orienta\u00e7\u00e3o do contribuinte que procura o atendimento da Receita Federal do Brasil (RFB)"},{"location":"index_old/#autores","text":"Adriano S. Zumba, Carlo C. Taglialegna e Reynaldo Assun\u00e7\u00e3o","title":"Autores"},{"location":"index_old/#resumo","text":"Este artigo se destina a propor o desenvolvimento de um assistente virtual (chatbot) que seja capaz de interagir com o contribuinte e direcion\u00e1-lo rapidamente ao servi\u00e7o demandado ou \u00e0s orienta\u00e7\u00f5es tribut\u00e1rias necess\u00e1rias, tendo em vista a grande quantidade de servi\u00e7os ofertados e os diversos canais de atendimento existentes na RFB. O objetivo do chatbot \u00e9 diminuir a dificuldade encontrada pelos contribuintes quando da demanda por um servi\u00e7o espec\u00edfico e evitar a navega\u00e7\u00e3o desnecess\u00e1ria pelas muitas p\u00e1ginas existentes no site da Receita Federal do Brasil. Palavras-chave: Chatbot; Atendimento; Receita Federal do Brasil.","title":"RESUMO"},{"location":"index_old/#abstract","text":"This article is intended to propose the development of a virtual assistant (chatbot) that is able to interact with the taxpayer and quickly direct him to the required service or the necessary tax guidelines, aiming at the large amount of services offered and the various channels service. The purpose of the chatbot is to reduce the difficulty encountered by taxpayers when they demand a specific service and to avoid unnecessary navigation through the many pages on the Receita Federal do Brasil website. Key words: Chatbot; Atendimento; Receita Federal. Data de submiss\u00e3o: dia.m\u00eas.ano Data de aprova\u00e7\u00e3o: dia.m\u00eas.ano Disponibilidade: (endere\u00e7o eletr\u00f4nico do artigo, DOI ou outras informa\u00e7\u00f5es).","title":"ABSTRACT"},{"location":"index_old/#1-introducao","text":"A Receita Federal do Brasil administra um n\u00famero expressivo de tributos e contribui\u00e7\u00f5es, o que implica em uma quantidade enorme de assuntos e servi\u00e7os relacionados a eles. A orienta\u00e7\u00e3o ao contribuinte sobre esses temas, bem como a presta\u00e7\u00e3o de uma quantidade enorme de servi\u00e7os dispon\u00edveis, cabe a uma rede de atendimento composta por cerca de uma dezena de canais de atendimento existentes na estrutura do \u00f3rg\u00e3o. A porta de entrada para o contribuinte \u00e9 o site da institui\u00e7\u00e3o, que pode ser acessado pelo endere\u00e7o https://www.gov.br/receitafederal/pt-br , o qual, a despeito de ser bastante organizado, contendo informa\u00e7\u00f5es detalhadas sobre cada servi\u00e7o/assunto, exige do contribuinte a navega\u00e7\u00e3o por muitas p\u00e1ginas dispostas em uma grande \u00e1rvore de menus, o que dificulta muito a busca pelo atendimento de que necessita. A exist\u00eancia dessa diversidade de canais de atendimento torna dif\u00edcil a identifica\u00e7\u00e3o do canal de atendimento apropriado para a realiza\u00e7\u00e3o de um determinado servi\u00e7o ou consulta. Muitas vezes, o contribuinte tem que percorrer v\u00e1rios desses canais at\u00e9 encontrar a solu\u00e7\u00e3o para o seu problema, numa busca intrincada que leva a uma certa perda de tempo. Por outro lado, as regras que estabelecem a compet\u00eancia de cada um dos canais de atendimento t\u00eam sido modificadas com muita frequ\u00eancia desde o advento da pandemia do novo coronav\u00edrus, ocasionando, muitas vezes, conflitos de compet\u00eancia entre as atividades desses canais. Com isso, observa-se uma certa inefici\u00eancia e demora no atendimento das solicita\u00e7\u00f5es dos contribuintes, gerando uma grande insatisfa\u00e7\u00e3o por parte dos mesmos. Como consequ\u00eancia, temos um aumento no n\u00famero de reclama\u00e7\u00f5es nos diversos canais de Ouvidoria. Em \u00e9pocas de restri\u00e7\u00f5es \u00e0 presen\u00e7a do contribuinte em nossas unidades de atendimento, como a que estamos vivendo, o atendimento virtual se apresenta como uma alternativa vi\u00e1vel para manter a disponibilidade na presta\u00e7\u00e3o de servi\u00e7os aos contribuintes. Some-se a isso o fechamento de muitos postos de atendimento presenciais. Dessa forma, o uso dessas tecnologias possibilita n\u00e3o s\u00f3 a alta disponibilidade dos servi\u00e7os, mas tamb\u00e9m a redu\u00e7\u00e3o dos custos operacionais associados. Diante do cen\u00e1rio aqui exposto, propomos o desenvolvimento de um assistente virtual (chatbot) que seja capaz de interagir com o contribuinte, de modo que ele possa encontrar rapidamente o servi\u00e7o demandado ou as orienta\u00e7\u00f5es tribut\u00e1rias de que necessita, sem ter que navegar pelas muitas p\u00e1ginas existentes no site da Receita Federal. Em nosso modelo proposto, o chatbot indicar\u00e1 onde o usu\u00e1rio obter orienta\u00e7\u00f5es tribut\u00e1rias e/ou informa\u00e7\u00f5es sobre o Servi\u00e7o que procura, para ter a sua solicita\u00e7\u00e3o atendida com rapidez e efici\u00eancia. A introdu\u00e7\u00e3o \u00e9 o primeiro elemento textual. Nela deve-se expor a finalidade e os objetivos do trabalho de modo que o leitor tenha uma vis\u00e3o do tema abordado. De modo geral, a introdu\u00e7\u00e3o deve apresentar: a) as caracter\u00edsticas do conte\u00fado a ser explorado; b) o objeto do trabalho e sua delimita\u00e7\u00e3o (delimitar de forma clara os limites da pesquisa); c) o est\u00e1gio do desenvolvimento do assunto, isto \u00e9, apresentar o que existe publicado sobre o assunto; d) o problema; e) o(s) objetivo(s); f) quando necess\u00e1rio, hip\u00f3teses ou vari\u00e1veis; g) justificativa do estudo; h) metodologia utilizada; i) refer\u00eancia as partes do trabalho e as j) possibilidades de contribui\u00e7\u00e3o da pesquisa, sem anunciar conclus\u00f5es e solu\u00e7\u00f5es (MEDEIROS, 2007).*","title":"1. Introdu\u00e7\u00e3o"},{"location":"index_old/#2-desenvolvimento","text":"Parte principal e mais extensa do trabalho deve apresentar a fundamenta\u00e7\u00e3o te\u00f3rica, a metodologia, os resultados e a discuss\u00e3o. Divide-se em se\u00e7\u00f5es e subse\u00e7\u00f5es conforme a NBR 6024 (ASSOCIA\u00c7\u00c3O BRASILEIRA DE NORMAS T\u00c9CNICAS, 2018a).","title":"2. Desenvolvimento"},{"location":"index_old/#3-consideracoes-finais","text":"Espera-se com esse projeto criar uma ferramenta que possua uma interface amig\u00e1vel e eficiente, que facilite ao cidad\u00e3o o atendimento de suas solicita\u00e7\u00f5es, fornecendo informa\u00e7\u00f5es de forma \u00e1gil e \u00fatil ao mesmo tempo, em conson\u00e2ncia com um dos pilares fundamentais dos princ\u00edpios que norteiam a administra\u00e7\u00e3o p\u00fablica: a efici\u00eancia. A democratiza\u00e7\u00e3o das informa\u00e7\u00f5es por meio de uma interface mais amig\u00e1vel \u00e9 fundamental para garantir a presta\u00e7\u00e3o de um atendimento de qualidade, reduzindo custos de trabalho adicionais por parte da administra\u00e7\u00e3o; propiciando padroniza\u00e7\u00e3o, conformidade e redu\u00e7\u00e3o de deslocamentos desnecess\u00e1rios do cidad\u00e3o; e uma maior disponibilidade de atendimento, que, no caso, seria em tempo integral. Portanto, o objetivo do presente trabalho \u00e9 demonstrar - sem, no entanto, esgotar o tema - que a partir de dados atuais disponibilizados em portais p\u00fablicos, como o da RFB, \u00e9 poss\u00edvel, com o uso de tecnologias e recursos de c\u00f3digo aberto, desenvolver um chatbot funcional que facilite a presta\u00e7\u00e3o eficiente de um servi\u00e7o de grande relev\u00e2ncia ao contribuinte, que \u00e9 uma orienta\u00e7\u00e3o tribut\u00e1ria adequada e informa\u00e7\u00f5es sobre como obter um determinado servi\u00e7o neste \u00d3rg\u00e3o, evitando assim perda de tempo e reclama\u00e7\u00f5es formais desnecess\u00e1rias. Parte final do artigo, onde se apresentam as conclus\u00f5es obtidas, mesmo que parciais. As conclus\u00f5es devem responder \u00e0s quest\u00f5es da pesquisa, correspondentes aos objetivos e hip\u00f3teses. Al\u00e9m disso, devem ser breves podendo apresentar recomenda\u00e7\u00f5es e sugest\u00f5es para trabalhos futuros.","title":"3. Considera\u00e7\u00f5es Finais"},{"location":"index_old/#referencias","text":"","title":"Refer\u00eancias"},{"location":"index_old/#glossario","text":"","title":"Gloss\u00e1rio"},{"location":"index_old/#apendice","text":"","title":"Ap\u00eandice"},{"location":"index_old/#anexos","text":"","title":"Anexos"},{"location":"index_old/#agradecimentos","text":"","title":"Agradecimentos"},{"location":"leonino/","text":"Aplicativo Web","title":"Aplicativo Web"},{"location":"leonino/#aplicativo-web","text":"","title":"Aplicativo Web"},{"location":"metodologia/","text":"Metodologia Extra\u00e7\u00e3o dos Dados Dados sobre os servi\u00e7os disponibilizados pela RFB e PGFN: Foram extra\u00eddos das p\u00e1ginas dispon\u00edveis em https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil e https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional. Para fazer a extra\u00e7\u00e3o atrav\u00e9s de Web Scraping, foi utilizada a biblioteca Python BeautifulSoup, com o c\u00f3digo sendo escrito em um Notebook Jupyter nomeado Dados com as Orienta\u00e7\u00f5es Tribut\u00e1rias da RFB: Extra\u00eddas de p\u00e1ginas dispon\u00edveis no Menu de Navega\u00e7\u00e3o do site da Receita Federal , op\u00e7\u00e3o \"Assuntos\", \"Mais Orienta\u00e7\u00f5es Tribut\u00e1rias\". Foram extra\u00eddos os assuntos dos seguintes itens deste Menu de Navega\u00e7\u00e3o: Cadastro; Certid\u00f5es e Situa\u00e7\u00e3o Fiscal Cobran\u00e7as e Intima\u00e7\u00f5es Declara\u00e7\u00f5es e Demonstrativos; Julgamento Administrativo; Pagamentos e Parcelamentos; Restitui\u00e7\u00e3o, Ressarcimento, Reembolso e Compensa\u00e7\u00e3o e Tributos N\u00c3O est\u00e3o incluidos neste prot\u00f3tipo muitos assuntos, tais como SIMPLES NACIONAL e MEI, ESOCIAL, PGFN, SPED, TRIBUTOS ADUANEIROS, dentre outros. A extra\u00e7\u00e3o foi feita em 2 etapas: Utilizou-se a biblioteca Scrapy para extrair os links que interessavam e que foram gravados em planilhas Numa segunda etapa foi utilizado a biblioteca BeautifulSoup para acessar cada link e extrair as informa\u00e7\u00f5es que ser\u00e3o utilizadas para compor o Banco de Perguntas e respostas, tamb\u00e9m salvos em planilhas Excel. Tratamento dos dados O pr\u00e9-processamento dos dados \u00e9 necess\u00e1rio para que os mesmos fiquem de forma a atender as necessidades dos modelos de Machine Learning que vamos utilizar. Em geral estima-se que 80% de todo projeto \u00e9 gasto nesta etapa. No nosso caso o pr\u00e9-processamento consistiu em: Limpeza dos dados, para remover espa\u00e7os e outros caracteres indesej\u00e1veis; Tokenizar o texto, de modo a extrair as palavras-chave que ser\u00e3o utilizadas nos modelos de NLP: converter o texto para letras min\u00fasculas; separar o textos em palavras; reduzir as palavras a sua raiz(Lemma) e remover as palavras sem significado sem\u00e2ntico (Stop Words) gravar na planilha todas as palavras significativas encontradas no texto Modelo de NLP utilizado Para trabalhar com os algoritmos de machine learning que ir\u00e3o comparar a pergunta do usu\u00e1rio com as que temos no nosso Banco de Dados, precisamos converter esse texto em n\u00fameros. Para isso utilizamos o algoritmo de \"Similaridade do Cosseno\" e duas t\u00e9cnicas, chamadas \"Bag of Words\" e \"TF-IDF\". Similaridade por Cosseno A similaridade por cosseno mede a similaridade entre dois vetores calculando o cosseno do \u00e2ngulo entre estes. \u00c9 uma das medidas de similaridade mais amplamente utilizadas e poderosas em Data Science. No nosso caso a utilizamos para estabelecer uma m\u00e9trica de semelhan\u00e7a entre frases/textos. Bag of Words O bag of words (saco de palavras) \u00e9 um modelo que representa as palavras de uma frase atrav\u00e9s de uma matriz num\u00e9rica, baseado apenas na frequ\u00eancia em que elas aparecem nesta frase, n\u00e3o considerando nenhuma informa\u00e7\u00e3o sobre a gram\u00e1tica ou a ordem em que as palavras aparecem no texto. TF-IDF Do ingl\u00eas Term Frequency - Inverse Document Frequency. Ele \u00e9 uma medida estat\u00edstica que indica a import\u00e2ncia de um termo de um documento em rela\u00e7\u00e3o a uma cole\u00e7\u00e3o de documentos. O tf-idf de um termo/palavra \u00e9 um \u00edndice proporcional a sua frequ\u00eancia dentro do pr\u00f3prio documento e inversamente proporcional a frequ\u00eancia em que ele aparece em todo o conjunto de documentos. A f\u00f3rmula matem\u00e1tica \u00e9: TF-IDF = TF x IDF, onde TF = (n\u00famero de vezes que o termo aparece em um documento) / (n\u00famero de termos no documento) , e IDF = log (N / n), em que N \u00e9 o n\u00famero de documentos e n \u00e9 o n\u00famero de documentos em que o termo apareceu. Assim, utilizando-se de bibliotecas python apropriadas, calculamos o TF-IDF das palavras chave da frase digitada pelo usu\u00e1rio, e comparamos com TF-IDF das palavras chave de cada uma das perguntas/respostas existentes em nosso banco de dados, indicando as que tem a maior similaridade com a d\u00favida do usu\u00e1rio. Para criar os modelos que utilizam a Similaridade por Cosseno e o TF-IDF e fornecem as resposta enviadas pelo ChatBot, foi utilizada a biblioteca Python Gensim.","title":"Metodologia"},{"location":"metodologia/#metodologia","text":"","title":"Metodologia"},{"location":"metodologia/#extracao-dos-dados","text":"","title":"Extra\u00e7\u00e3o dos Dados"},{"location":"metodologia/#dados-sobre-os-servicos-disponibilizados-pela-rfb-e-pgfn","text":"Foram extra\u00eddos das p\u00e1ginas dispon\u00edveis em https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil e https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional. Para fazer a extra\u00e7\u00e3o atrav\u00e9s de Web Scraping, foi utilizada a biblioteca Python BeautifulSoup, com o c\u00f3digo sendo escrito em um Notebook Jupyter nomeado","title":"Dados sobre os servi\u00e7os disponibilizados pela RFB e PGFN:"},{"location":"metodologia/#dados-com-as-orientacoes-tributarias-da-rfb","text":"Extra\u00eddas de p\u00e1ginas dispon\u00edveis no Menu de Navega\u00e7\u00e3o do site da Receita Federal , op\u00e7\u00e3o \"Assuntos\", \"Mais Orienta\u00e7\u00f5es Tribut\u00e1rias\". Foram extra\u00eddos os assuntos dos seguintes itens deste Menu de Navega\u00e7\u00e3o: Cadastro; Certid\u00f5es e Situa\u00e7\u00e3o Fiscal Cobran\u00e7as e Intima\u00e7\u00f5es Declara\u00e7\u00f5es e Demonstrativos; Julgamento Administrativo; Pagamentos e Parcelamentos; Restitui\u00e7\u00e3o, Ressarcimento, Reembolso e Compensa\u00e7\u00e3o e Tributos N\u00c3O est\u00e3o incluidos neste prot\u00f3tipo muitos assuntos, tais como SIMPLES NACIONAL e MEI, ESOCIAL, PGFN, SPED, TRIBUTOS ADUANEIROS, dentre outros. A extra\u00e7\u00e3o foi feita em 2 etapas: Utilizou-se a biblioteca Scrapy para extrair os links que interessavam e que foram gravados em planilhas Numa segunda etapa foi utilizado a biblioteca BeautifulSoup para acessar cada link e extrair as informa\u00e7\u00f5es que ser\u00e3o utilizadas para compor o Banco de Perguntas e respostas, tamb\u00e9m salvos em planilhas Excel.","title":"Dados com as Orienta\u00e7\u00f5es Tribut\u00e1rias da RFB:"},{"location":"metodologia/#tratamento-dos-dados","text":"O pr\u00e9-processamento dos dados \u00e9 necess\u00e1rio para que os mesmos fiquem de forma a atender as necessidades dos modelos de Machine Learning que vamos utilizar. Em geral estima-se que 80% de todo projeto \u00e9 gasto nesta etapa. No nosso caso o pr\u00e9-processamento consistiu em: Limpeza dos dados, para remover espa\u00e7os e outros caracteres indesej\u00e1veis; Tokenizar o texto, de modo a extrair as palavras-chave que ser\u00e3o utilizadas nos modelos de NLP: converter o texto para letras min\u00fasculas; separar o textos em palavras; reduzir as palavras a sua raiz(Lemma) e remover as palavras sem significado sem\u00e2ntico (Stop Words) gravar na planilha todas as palavras significativas encontradas no texto","title":"Tratamento dos dados"},{"location":"metodologia/#modelo-de-nlp-utilizado","text":"Para trabalhar com os algoritmos de machine learning que ir\u00e3o comparar a pergunta do usu\u00e1rio com as que temos no nosso Banco de Dados, precisamos converter esse texto em n\u00fameros. Para isso utilizamos o algoritmo de \"Similaridade do Cosseno\" e duas t\u00e9cnicas, chamadas \"Bag of Words\" e \"TF-IDF\".","title":"Modelo de NLP utilizado"},{"location":"metodologia/#similaridade-por-cosseno","text":"A similaridade por cosseno mede a similaridade entre dois vetores calculando o cosseno do \u00e2ngulo entre estes. \u00c9 uma das medidas de similaridade mais amplamente utilizadas e poderosas em Data Science. No nosso caso a utilizamos para estabelecer uma m\u00e9trica de semelhan\u00e7a entre frases/textos.","title":"Similaridade por Cosseno"},{"location":"metodologia/#bag-of-words","text":"O bag of words (saco de palavras) \u00e9 um modelo que representa as palavras de uma frase atrav\u00e9s de uma matriz num\u00e9rica, baseado apenas na frequ\u00eancia em que elas aparecem nesta frase, n\u00e3o considerando nenhuma informa\u00e7\u00e3o sobre a gram\u00e1tica ou a ordem em que as palavras aparecem no texto.","title":"Bag of Words"},{"location":"metodologia/#tf-idf","text":"Do ingl\u00eas Term Frequency - Inverse Document Frequency. Ele \u00e9 uma medida estat\u00edstica que indica a import\u00e2ncia de um termo de um documento em rela\u00e7\u00e3o a uma cole\u00e7\u00e3o de documentos. O tf-idf de um termo/palavra \u00e9 um \u00edndice proporcional a sua frequ\u00eancia dentro do pr\u00f3prio documento e inversamente proporcional a frequ\u00eancia em que ele aparece em todo o conjunto de documentos. A f\u00f3rmula matem\u00e1tica \u00e9: TF-IDF = TF x IDF, onde TF = (n\u00famero de vezes que o termo aparece em um documento) / (n\u00famero de termos no documento) , e IDF = log (N / n), em que N \u00e9 o n\u00famero de documentos e n \u00e9 o n\u00famero de documentos em que o termo apareceu. Assim, utilizando-se de bibliotecas python apropriadas, calculamos o TF-IDF das palavras chave da frase digitada pelo usu\u00e1rio, e comparamos com TF-IDF das palavras chave de cada uma das perguntas/respostas existentes em nosso banco de dados, indicando as que tem a maior similaridade com a d\u00favida do usu\u00e1rio. Para criar os modelos que utilizam a Similaridade por Cosseno e o TF-IDF e fornecem as resposta enviadas pelo ChatBot, foi utilizada a biblioteca Python Gensim.","title":"TF-IDF"},{"location":"palavras_chave/","text":"Extra\u00e7\u00e3o das palavras chave dos servi\u00e7os Nome do notebook jupyter: 02extrai_palavras_chave.ypnb : Extraindo as senten\u00e7as principais do texto oQueE dos servi\u00e7os RFB Importando as bibliotecas #IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS import nltk import time import pandas as pd import os import re from nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from string import punctuation from nltk.probability import FreqDist from collections import defaultdict from heapq import nlargest # Extra\u00e7\u00e3o das palavras chaves das informa\u00e7\u00f5es de cada servi\u00e7o # c\u00f3digo para verificar as senten\u00e7as mais importantes no texto da descri\u00e7\u00e3o do servi\u00e7o, texto do qual # ser\u00e3o extra\u00eddas as palavras chave para utiliza\u00e7\u00e3o nos modelos utilizados pelo chatbot # # lista com as stopwords, ou seja, palavras n\u00e3o importantes e que podem ser descartadas stopwords1 = stopwords.words('portuguese') + list(punctuation) + ['\u2013'] plx = ['``',\"''\"] # caracteres de pontua\u00e7\u00e3o tamb\u00e9m indesej\u00e1veis # l\u00ea a planilha gerada anteriormente, com os dados dos servi\u00e7os df1 = pd.read_excel('./ServicosRFBePGFN_Final.xlsx') # chave_lst = [] # lista para guardar as palavras chave for i in range(len(df1)): # loop que itera sobre cada servi\u00e7o para extrair as frases mais importantes do texto sentencas = sent_tokenize(df1.iloc[i][2]) # separando o texto em senten\u00e7as texto = \"\" for j in sentencas: # converte a lista com as senten\u00e7as em string texto = texto + j + \" \" texto = texto.strip().lower() # transforma o texto em min\u00fasculas palavras = word_tokenize(texto) # tokeniza o texto em palavras palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords1] # retira as stopwords do texto palavras_sem_stopwords1 = [] for pal in palavras_sem_stopwords: #retira os caracteres indesej\u00e1veis do texto if pal not in plx: palavras_sem_stopwords1.append(pal) palavras_sem_stopwords = palavras_sem_stopwords1 frequencia = FreqDist(palavras_sem_stopwords) # cria a distribui\u00e7\u00e3o de frequ\u00eancia das palavras sentencas_importantes = defaultdict(int) # cria dicion\u00e1rio que conter\u00e1 as senten\u00e7as mais importantes for k, sentenca in enumerate(sentencas): #calcula a pontua\u00e7\u00e3o de cadas senten\u00e7a, baseado na frequ\u00eancia das palavras for palavra in word_tokenize(sentenca.lower()): if palavra in frequencia: sentencas_importantes[k] += frequencia[palavra] # separa as 3 senten\u00e7as com maior pontua\u00e7\u00e3o idx_sentencas_importantes = nlargest(3, sentencas_importantes, sentencas_importantes.get) #atribui peso 4 ao t\u00edtulo do servi\u00e7o e peso 3 para a senten\u00e7a mais importante do texto da descri\u00e7\u00e3o do servi\u00e7o str_chave = str(df1.iloc[i][0]+' ')*4 + (sentencas[0]+ ' ')*3 # string com as palavras chave for y in sorted(idx_sentencas_importantes): # acrescenta as senten\u00e7as importantes na string de palavras chave str_chave = str_chave + \" \" + sentencas[y] chave_lst.append(str_chave) # acrescenta a string de palavras chave a lista # # para compor a lista das palavras chaves, foi atribuido peso 4 para o t\u00edtulo do servi\u00e7o, peso 4 para a # frase mais importante e peso 1 para as outras duas frases importantes # Grava os servi\u00e7os em uma planilha df1['texto_chave'] = chave_lst df1.to_excel('./ServicosRfbFinal.xlsx', index = False) # len(df1) # n\u00famero de servi\u00e7os da base de dados = 181 Carrega os dataframes com o conte\u00fado das duas planilhas df1 = pd.read_excel('./ServicosRfbFinal.xlsx') df2 = pd.read_excel('./PerguntasTreinamento.xlsx') # planilha criada manualmente com perguntas/respostas comuns # carrega bibliotecas de NLP, spacy e nltk import spacy nlp = spacy.load(\"pt_core_news_md\") #carrega o corpus em portugu\u00eas do spacy from nltk.stem import RSLPStemmer from nltk.tokenize import sent_tokenize, word_tokenize nltk.download('rslp') ste = nltk.stem.RSLPStemmer() Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chave dos servi\u00e7os def troca_acentos(txt): #fun\u00e7\u00e3o que retira os acentos da l\u00edngua transTable = txt.maketrans(\"\u00e1\u00e0\u00e3\u00e9\u00ea\u00ed\u00f3\u00f4\u00f5\u00fa\u00c1\u00c3\u00c3\u00c9\u00ca\u00cd\u00d3\u00d4\u00d5\u00da\u00e7\u00c7\", \"aaaeeiooouaaaeeioooucc\") txt = txt.translate(transTable) return str(txt) # # # fun\u00e7\u00e3o que tokeniza as palavras # def tokenizar(texto): # fun\u00e7\u00e3o que tokeniza o texto, e deixa somente palavras significativas # car_nok = ['SPACE', 'DET','ADP', 'CCONJ','PUNCT','PRON','SCONJ'] txx = texto docxx = nlp(txx) txx_lemma = [(token.text,token.orth_, token.lemma_, token.pos_) for token in docxx] #reduz a palavra ao seu Lemma pal_xx = '' plvxx = \"\" for i in txx_lemma: if i[3] not in car_nok and len(i[0]) > 2: plvxx = plvxx + ste.stem(i[0]) + ' ' plvxx = plvxx.strip() plvxx = troca_acentos(plvxx) return plvxx # fim funcao tokenizar # # fun\u00e7\u00e3o para limpar o texto def limpa_texto(txt_a_limpar): # transforma em min\u00fasculas e retira espa;os iniciais e finais txt_ok = txt_a_limpar.lower() txt_ok = txt_ok.strip() return txt_ok # # fim da funcao limpa_texto # # # cria planilha com as palavras chave dos servi\u00e7os print(\"***inicio tokeniza\u00e7\u00e3o dos servi\u00e7os***\") plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df1)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df1.iloc[j][3] txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df1['chaves_serv'] = plvx1 # # print(\"#### fim tokeniza\u00e7\u00e3o dos servi\u00e7os ####\") # Grava vers\u00e3o final da planilha de servi\u00e7os # grava as palavras chave na planilha de servi\u00e7os df1.to_excel('./ServicosRfbFinal2.xlsx', index = False) Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chaves das frases de conversas comuns print(\"### inicio tokeniza\u00e7\u00e3o das conversas ###\") # cria planilha com as palavras chave dos servi\u00e7os plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df2)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df2.iloc[j][0] + ' ' txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df2['chaves_serv'] = plvx1 # print(\"### fim tokeniza\u00e7\u00e3o das conversas ###\")","title":"Extra\u00e7\u00e3o das Palavras-chave"},{"location":"palavras_chave/#extracao-das-palavras-chave-dos-servicos","text":"Nome do notebook jupyter: 02extrai_palavras_chave.ypnb :","title":"Extra\u00e7\u00e3o das palavras chave dos servi\u00e7os"},{"location":"palavras_chave/#extraindo-as-sentencas-principais-do-texto-oquee-dos-servicos-rfb","text":"","title":"Extraindo as senten\u00e7as principais do texto oQueE dos servi\u00e7os RFB"},{"location":"palavras_chave/#importando-as-bibliotecas","text":"#IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS import nltk import time import pandas as pd import os import re from nltk.tokenize import word_tokenize from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from string import punctuation from nltk.probability import FreqDist from collections import defaultdict from heapq import nlargest #","title":"Importando as bibliotecas"},{"location":"palavras_chave/#extracao-das-palavras-chaves-das-informacoes-de-cada-servico","text":"# c\u00f3digo para verificar as senten\u00e7as mais importantes no texto da descri\u00e7\u00e3o do servi\u00e7o, texto do qual # ser\u00e3o extra\u00eddas as palavras chave para utiliza\u00e7\u00e3o nos modelos utilizados pelo chatbot # # lista com as stopwords, ou seja, palavras n\u00e3o importantes e que podem ser descartadas stopwords1 = stopwords.words('portuguese') + list(punctuation) + ['\u2013'] plx = ['``',\"''\"] # caracteres de pontua\u00e7\u00e3o tamb\u00e9m indesej\u00e1veis # l\u00ea a planilha gerada anteriormente, com os dados dos servi\u00e7os df1 = pd.read_excel('./ServicosRFBePGFN_Final.xlsx') # chave_lst = [] # lista para guardar as palavras chave for i in range(len(df1)): # loop que itera sobre cada servi\u00e7o para extrair as frases mais importantes do texto sentencas = sent_tokenize(df1.iloc[i][2]) # separando o texto em senten\u00e7as texto = \"\" for j in sentencas: # converte a lista com as senten\u00e7as em string texto = texto + j + \" \" texto = texto.strip().lower() # transforma o texto em min\u00fasculas palavras = word_tokenize(texto) # tokeniza o texto em palavras palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords1] # retira as stopwords do texto palavras_sem_stopwords1 = [] for pal in palavras_sem_stopwords: #retira os caracteres indesej\u00e1veis do texto if pal not in plx: palavras_sem_stopwords1.append(pal) palavras_sem_stopwords = palavras_sem_stopwords1 frequencia = FreqDist(palavras_sem_stopwords) # cria a distribui\u00e7\u00e3o de frequ\u00eancia das palavras sentencas_importantes = defaultdict(int) # cria dicion\u00e1rio que conter\u00e1 as senten\u00e7as mais importantes for k, sentenca in enumerate(sentencas): #calcula a pontua\u00e7\u00e3o de cadas senten\u00e7a, baseado na frequ\u00eancia das palavras for palavra in word_tokenize(sentenca.lower()): if palavra in frequencia: sentencas_importantes[k] += frequencia[palavra] # separa as 3 senten\u00e7as com maior pontua\u00e7\u00e3o idx_sentencas_importantes = nlargest(3, sentencas_importantes, sentencas_importantes.get) #atribui peso 4 ao t\u00edtulo do servi\u00e7o e peso 3 para a senten\u00e7a mais importante do texto da descri\u00e7\u00e3o do servi\u00e7o str_chave = str(df1.iloc[i][0]+' ')*4 + (sentencas[0]+ ' ')*3 # string com as palavras chave for y in sorted(idx_sentencas_importantes): # acrescenta as senten\u00e7as importantes na string de palavras chave str_chave = str_chave + \" \" + sentencas[y] chave_lst.append(str_chave) # acrescenta a string de palavras chave a lista # # para compor a lista das palavras chaves, foi atribuido peso 4 para o t\u00edtulo do servi\u00e7o, peso 4 para a # frase mais importante e peso 1 para as outras duas frases importantes #","title":"Extra\u00e7\u00e3o das palavras chaves das informa\u00e7\u00f5es de cada servi\u00e7o"},{"location":"palavras_chave/#grava-os-servicos-em-uma-planilha","text":"df1['texto_chave'] = chave_lst df1.to_excel('./ServicosRfbFinal.xlsx', index = False) # len(df1) # n\u00famero de servi\u00e7os da base de dados = 181","title":"Grava os servi\u00e7os em uma planilha"},{"location":"palavras_chave/#carrega-os-dataframes-com-o-conteudo-das-duas-planilhas","text":"df1 = pd.read_excel('./ServicosRfbFinal.xlsx') df2 = pd.read_excel('./PerguntasTreinamento.xlsx') # planilha criada manualmente com perguntas/respostas comuns #","title":"Carrega os dataframes com o conte\u00fado das duas planilhas"},{"location":"palavras_chave/#carrega-bibliotecas-de-nlp-spacy-e-nltk","text":"import spacy nlp = spacy.load(\"pt_core_news_md\") #carrega o corpus em portugu\u00eas do spacy from nltk.stem import RSLPStemmer from nltk.tokenize import sent_tokenize, word_tokenize nltk.download('rslp') ste = nltk.stem.RSLPStemmer()","title":"carrega bibliotecas de NLP, spacy e nltk"},{"location":"palavras_chave/#tokenizacao-e-extracao-das-palavras-chave-dos-servicos","text":"def troca_acentos(txt): #fun\u00e7\u00e3o que retira os acentos da l\u00edngua transTable = txt.maketrans(\"\u00e1\u00e0\u00e3\u00e9\u00ea\u00ed\u00f3\u00f4\u00f5\u00fa\u00c1\u00c3\u00c3\u00c9\u00ca\u00cd\u00d3\u00d4\u00d5\u00da\u00e7\u00c7\", \"aaaeeiooouaaaeeioooucc\") txt = txt.translate(transTable) return str(txt) # # # fun\u00e7\u00e3o que tokeniza as palavras # def tokenizar(texto): # fun\u00e7\u00e3o que tokeniza o texto, e deixa somente palavras significativas # car_nok = ['SPACE', 'DET','ADP', 'CCONJ','PUNCT','PRON','SCONJ'] txx = texto docxx = nlp(txx) txx_lemma = [(token.text,token.orth_, token.lemma_, token.pos_) for token in docxx] #reduz a palavra ao seu Lemma pal_xx = '' plvxx = \"\" for i in txx_lemma: if i[3] not in car_nok and len(i[0]) > 2: plvxx = plvxx + ste.stem(i[0]) + ' ' plvxx = plvxx.strip() plvxx = troca_acentos(plvxx) return plvxx # fim funcao tokenizar # # fun\u00e7\u00e3o para limpar o texto def limpa_texto(txt_a_limpar): # transforma em min\u00fasculas e retira espa;os iniciais e finais txt_ok = txt_a_limpar.lower() txt_ok = txt_ok.strip() return txt_ok # # fim da funcao limpa_texto # # # cria planilha com as palavras chave dos servi\u00e7os print(\"***inicio tokeniza\u00e7\u00e3o dos servi\u00e7os***\") plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df1)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df1.iloc[j][3] txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df1['chaves_serv'] = plvx1 # # print(\"#### fim tokeniza\u00e7\u00e3o dos servi\u00e7os ####\") #","title":"Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chave dos servi\u00e7os"},{"location":"palavras_chave/#grava-versao-final-da-planilha-de-servicos","text":"# grava as palavras chave na planilha de servi\u00e7os df1.to_excel('./ServicosRfbFinal2.xlsx', index = False)","title":"Grava vers\u00e3o final da planilha de servi\u00e7os"},{"location":"palavras_chave/#tokenizacao-e-extracao-das-palavras-chaves-das-frases-de-conversas-comuns","text":"print(\"### inicio tokeniza\u00e7\u00e3o das conversas ###\") # cria planilha com as palavras chave dos servi\u00e7os plvx1 = [] # loop para percorrer o dataframe e extrair as palavras chave for j in range(len(df2)): # composi\u00e7\u00e3o do texto = 3 vezes palavras do t\u00edtulo descrevendo o servi\u00e7o # sup\u00f5e-se serem as palavras mais importantes, por isso deu-se a elas o peso 5 # as palavras explicando o que \u00e9 o servi\u00e7o (oQue) tamb\u00e9m ser\u00e3o extra\u00eddas txt = df2.iloc[j][0] + ' ' txt_token = tokenizar(limpa_texto(txt)) plvx1.append(txt_token) # acrescenta coluna com as palavras chave ao dataframe df2['chaves_serv'] = plvx1 # print(\"### fim tokeniza\u00e7\u00e3o das conversas ###\")","title":"Tokeniza\u00e7\u00e3o e extra\u00e7\u00e3o das palavras chaves das frases de conversas comuns"},{"location":"questionario/","text":"Question\u00e1rio de avalia\u00e7\u00e3o da Ferramenta Assistente Virtual(ChatBot) para Orienta\u00e7\u00e3o Tribut\u00e1ria e sobre Servi\u00e7os da Receita Federal do Brasil Question\u00e1rio para Avalia\u00e7\u00e3o da Ferramenta Prezado amigo(a) usu\u00e1rio(a): Este formul\u00e1rio se presta a avalia\u00e7\u00e3o do nosso trabalho/Artigo Cient\u00edfico que ser\u00e1 pr\u00e9-requisito para a conclus\u00e3o do nosso Curso de Especializa\u00e7\u00e3o em Ci\u00eancia de Dados - ECD - FURG. O nosso projeto foi desenvolver um aplicativo que possa responder as d\u00favidas dos usu\u00e1rios de servi\u00e7os da RFB, de modo que tenha facilidade em tirar uma d\u00favida sobre uma orienta\u00e7\u00e3o tribut\u00e1ria ou sobre um servi\u00e7o espec\u00edfico, sem ter que navegar pelos Menus de op\u00e7\u00f5es dispon\u00edveis no site da Receita Federal do Brasil. Para nos ajudar e contribuir com o nosso trabalho, primeiro deve acessar o site do aplicativo e digitar uma frase com a sua d\u00favida sobre a orienta\u00e7\u00e3o tribut\u00e1ria ou sobre o servi\u00e7o, da seguinte forma: se quer tirar a d\u00favida sobre um servi\u00e7o, digite antes da frase o caracter \"?\" se quer uma orienta\u00e7\u00e3o tribut\u00e1ria, digite antes da frase o caracter \"=\" se n\u00e3o ficar satisfeito com a resposta para a sua d\u00favida, pode digitar o caracter \"#\" seguido da frase, que o aplicativo far\u00e1 uma pesquisa no site da Receita para tentar encontrar o que precisa Ap\u00f3s fazer fazer algumas perguntas ao nosso Assistente, pe\u00e7o que responda ao seguinte question\u00e1rio: O chatbot \u00e9 f\u00e1cil de utilizar? SIM/N\u00c3O Encontrei as respostas para as minhas d\u00favidas foram satisfat\u00f3rias? 1-para mais da metade das perguntas/ 2-para menos de metade / 0-para nenhuma pergunta De 0(zero) a 5(cinco) , qual a nota voc\u00ea daria para o assistente? Observa\u00e7\u00f5es: Favor enviar o question\u00e1rio para carlomcz@gmail.com Para testar o aplicativo, basta clicar aqui Nesta vers\u00e3o inicial s\u00f3 est\u00e3o inclu\u00eddas respostas para os seguintes assuntos: Servi\u00e7os da RFB e da PGFN dispon\u00edveis em https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil e https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional Orienta\u00e7\u00e3o Tribut\u00e1ria sobre alguns dos assuntos dispon\u00edveis no Menu de Navega\u00e7\u00e3o do site da Receita Federal , op\u00e7\u00e3o \"Assuntos\", \"Mais Orienta\u00e7\u00f5es Tribut\u00e1rias\": Cadastro; Certid\u00f5es e Situa\u00e7\u00e3o Fiscal; Cobran\u00e7as e Intima\u00e7\u00f5es; Declara\u00e7\u00f5es e Demonstrativos; Julgamento Administrativo; Pagamentos e Parcelamentos; Restitui\u00e7\u00e3o, Ressarcimento, Reembolso e Compensa\u00e7\u00e3o e Tributos N\u00c3O est\u00e3o incluidos por exemplo os assuntos SIMPLES NACIONAL e MEI, ESOCIAL, PGFN, SPED, TRIBUTOS ADUANEIROS, entre outros","title":"Questionario"},{"location":"questionario/#questionario-de-avaliacao-da-ferramenta","text":"","title":"Question\u00e1rio de avalia\u00e7\u00e3o da Ferramenta"},{"location":"questionario/#assistente-virtualchatbot-para-orientacao-tributaria-e-sobre-servicos-da-receita-federal-do-brasil","text":"","title":"Assistente Virtual(ChatBot) para Orienta\u00e7\u00e3o Tribut\u00e1ria e sobre Servi\u00e7os da Receita Federal do Brasil"},{"location":"questionario/#questionario-para-avaliacao-da-ferramenta","text":"Prezado amigo(a) usu\u00e1rio(a): Este formul\u00e1rio se presta a avalia\u00e7\u00e3o do nosso trabalho/Artigo Cient\u00edfico que ser\u00e1 pr\u00e9-requisito para a conclus\u00e3o do nosso Curso de Especializa\u00e7\u00e3o em Ci\u00eancia de Dados - ECD - FURG. O nosso projeto foi desenvolver um aplicativo que possa responder as d\u00favidas dos usu\u00e1rios de servi\u00e7os da RFB, de modo que tenha facilidade em tirar uma d\u00favida sobre uma orienta\u00e7\u00e3o tribut\u00e1ria ou sobre um servi\u00e7o espec\u00edfico, sem ter que navegar pelos Menus de op\u00e7\u00f5es dispon\u00edveis no site da Receita Federal do Brasil. Para nos ajudar e contribuir com o nosso trabalho, primeiro deve acessar o site do aplicativo e digitar uma frase com a sua d\u00favida sobre a orienta\u00e7\u00e3o tribut\u00e1ria ou sobre o servi\u00e7o, da seguinte forma: se quer tirar a d\u00favida sobre um servi\u00e7o, digite antes da frase o caracter \"?\" se quer uma orienta\u00e7\u00e3o tribut\u00e1ria, digite antes da frase o caracter \"=\" se n\u00e3o ficar satisfeito com a resposta para a sua d\u00favida, pode digitar o caracter \"#\" seguido da frase, que o aplicativo far\u00e1 uma pesquisa no site da Receita para tentar encontrar o que precisa Ap\u00f3s fazer fazer algumas perguntas ao nosso Assistente, pe\u00e7o que responda ao seguinte question\u00e1rio: O chatbot \u00e9 f\u00e1cil de utilizar? SIM/N\u00c3O Encontrei as respostas para as minhas d\u00favidas foram satisfat\u00f3rias? 1-para mais da metade das perguntas/ 2-para menos de metade / 0-para nenhuma pergunta De 0(zero) a 5(cinco) , qual a nota voc\u00ea daria para o assistente?","title":"Question\u00e1rio para Avalia\u00e7\u00e3o da Ferramenta"},{"location":"questionario/#observacoes","text":"Favor enviar o question\u00e1rio para carlomcz@gmail.com Para testar o aplicativo, basta clicar aqui Nesta vers\u00e3o inicial s\u00f3 est\u00e3o inclu\u00eddas respostas para os seguintes assuntos: Servi\u00e7os da RFB e da PGFN dispon\u00edveis em https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil e https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional Orienta\u00e7\u00e3o Tribut\u00e1ria sobre alguns dos assuntos dispon\u00edveis no Menu de Navega\u00e7\u00e3o do site da Receita Federal , op\u00e7\u00e3o \"Assuntos\", \"Mais Orienta\u00e7\u00f5es Tribut\u00e1rias\": Cadastro; Certid\u00f5es e Situa\u00e7\u00e3o Fiscal; Cobran\u00e7as e Intima\u00e7\u00f5es; Declara\u00e7\u00f5es e Demonstrativos; Julgamento Administrativo; Pagamentos e Parcelamentos; Restitui\u00e7\u00e3o, Ressarcimento, Reembolso e Compensa\u00e7\u00e3o e Tributos N\u00c3O est\u00e3o incluidos por exemplo os assuntos SIMPLES NACIONAL e MEI, ESOCIAL, PGFN, SPED, TRIBUTOS ADUANEIROS, entre outros","title":"Observa\u00e7\u00f5es:"},{"location":"raspagem/","text":"Extra\u00e7\u00e3o/Raspagem dos Textos A coleta dos dados foi realizada atrav\u00e9s de c\u00f3digo desenvolvido em Python e executados no Jupyter Notebook, em um notebook de nome 01raspa_servicos.ypnb. Abaixo mostraremos o c\u00f3digo com coment\u00e1rios explicando a l\u00f3gica. Importa\u00e7\u00e3o das bibliotecas Python necess\u00e1rias #IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS from bs4 import BeautifulSoup import time import requests import urllib # importanto a biblioteca pandas import pandas as pd from urllib.request import urlopen # A biblioteca BeautifulSoup \u00e9 que foi utilizada para fazer o Web Scraping(raspagem dos dados). Gerando uma lista com as p\u00e1ginas de onde ser\u00e3o \"raspados\" os links dos servi\u00e7os # #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da RFB no .gov base1 = 'https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil?b_start:int=' #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da PGFN no .gov base2 = 'https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional?b_start:int=' # gerando lista com as p\u00e1ginas de onde ser\u00e3o extra\u00eddos os links para cada servi\u00e7o existente para os dois \u00f3rg\u00e3os, RFB e PGFN lista_pgs = [] y = 0 for i in range(6): # seis p\u00e1ginas com links de servi\u00e7os da RFB if i > 0: y = y + 30 lista_pgs.append(base1 + str(y)) y = 0 for i in range(2): # duas p\u00e1ginas com links de servi\u00e7os da PGFN if i > 0: y = y + 30 lista_pgs.append(base2 + str(y)) # Fazendo o scraping das informa\u00e7\u00f5es sobre cada servi\u00e7o # para cada link na lista dos servi\u00e7os, faz o Scraping (raspagem) dos t\u00edtulos e links de cada servi\u00e7o l_lnk = [] # lista para armazenar os links de cada servi\u00e7o l_titulo = [] # lista para armazenas o titulo do servi\u00e7o # for i in lista_pgs: # loop que itera sobre cada p\u00e1gina de onde ser\u00e3o extra\u00eddos os links dos servi\u00e7os try: html = urlopen(i) # acessa a p\u00e1gina que contem os links bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup com o conte\u00fado da p\u00e1gina pg = bs.find('ul', class_='listagem') # encontra a 'tag' <ul> , com classe listagem, que contem os links dos servi\u00e7os itens = pg.find_all('li', class_='item') # encontra a 'tag ' <li> , com classe item, onde est\u00e1 cada link ('tag' <a>) for j in itens: # loop sobre cada item, para extrair o t\u00edtulo, o link e a categoria de cada servi\u00e7o ln1 = j.find('a') # 'tag' <a> que contem o link de cada servi\u00e7o titx = ln1['title'] # atributo 't\u00edtulo' do servi\u00e7o lnx = ln1['href'] # atributo 'href' do servi\u00e7o (link) l_titulo.append(titx) # acrescenta o t\u00edtulo do servi\u00e7o a lista l_lnk.append(lnx) # acrescenta o link para o servi\u00e7o a lista except: #se houver erro n\u00e3o interrompe o programa pass Utilizando a biblioteca Pandas para gravar os dados \"raspados\" em uma planilha excel # grava uma planilha excel com os titulos e os links para cada servi\u00e7o, que ser\u00e3o utilizados para acessar # cada p\u00e1gina do servi\u00e7o e fazer a \"raspagem\" da descri\u00e7\u00e3o do servi\u00e7o dfx = pd.DataFrame({'Servico': l_titulo ,'Link': l_lnk}) dfx.to_excel('./ServicosRFBePGFN.xlsx', index = False) Acessa cada link gravado anteriormente para \"raspar\" o texto que descreve cada servi\u00e7o # c\u00f3digo que ir\u00e1 acessar cada link do servi\u00e7o para fazer a \"raspagem\" da texto que descreve o servi\u00e7o links = dfx['Link'] # lista com os links oques = [] # lista onde ser\u00e1 armazenada a descri\u00e7\u00e3o e cada um dos servi\u00e7os for i in range(len(links)): # itera sobre cada link e acessa a p\u00e1gina do servi\u00e7o try: if i < 0: # teste para fazer a raspagem a partir de um item da lista, com 'zero' raspa todos os itens print(\"#\", i) continue html = urlopen(str(links[i])) # acessa a p\u00e1gina do servi\u00e7o print(\"#\", i) # imprime o nr. do item e o link, apenas para visualizar como est\u00e1 o processamento except: continue bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup da p\u00e1gina print(links[i]) div_que = bs.find('div', class_='conteudo') # procura a 'tag' <div> onde est\u00e1 a descri\u00e7\u00e3o do servi\u00e7o texto = div_que.text # texto com a descri\u00e7\u00e3o de cada servi\u00e7o oques.append(texto) # adiciona a descri\u00e7\u00e3o do servi\u00e7o a lista time.sleep(3) # tempo de espera para n\u00e3o sobrecarregar o servidor .gov ## Acrescenta a planilha gravada o atributo com o texto que descreve o servi\u00e7o # # acrescenta a planilha criada anteriormente o atributo que descreve o servi\u00e7o dfx['oQue'] = oques # #Salvando a planilha # dfx.to_excel('./ServicosRFBePGFN_Final.xlsx', index = False) #","title":"Extra\u00e7\u00e3o dos Textos"},{"location":"raspagem/#extracaoraspagem-dos-textos","text":"A coleta dos dados foi realizada atrav\u00e9s de c\u00f3digo desenvolvido em Python e executados no Jupyter Notebook, em um notebook de nome 01raspa_servicos.ypnb. Abaixo mostraremos o c\u00f3digo com coment\u00e1rios explicando a l\u00f3gica.","title":"Extra\u00e7\u00e3o/Raspagem dos Textos"},{"location":"raspagem/#importacao-das-bibliotecas-python-necessarias","text":"#IMPORTANDO AS BIBLIOTECAS NECESS\u00c1RIAS from bs4 import BeautifulSoup import time import requests import urllib # importanto a biblioteca pandas import pandas as pd from urllib.request import urlopen # A biblioteca BeautifulSoup \u00e9 que foi utilizada para fazer o Web Scraping(raspagem dos dados).","title":"Importa\u00e7\u00e3o das bibliotecas Python necess\u00e1rias"},{"location":"raspagem/#gerando-uma-lista-com-as-paginas-de-onde-serao-raspados-os-links-dos-servicos","text":"# #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da RFB no .gov base1 = 'https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil?b_start:int=' #p\u00e1gina base de onde ser\u00e3o extra\u00eddos os links para os servi\u00e7os da PGFN no .gov base2 = 'https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional?b_start:int=' # gerando lista com as p\u00e1ginas de onde ser\u00e3o extra\u00eddos os links para cada servi\u00e7o existente para os dois \u00f3rg\u00e3os, RFB e PGFN lista_pgs = [] y = 0 for i in range(6): # seis p\u00e1ginas com links de servi\u00e7os da RFB if i > 0: y = y + 30 lista_pgs.append(base1 + str(y)) y = 0 for i in range(2): # duas p\u00e1ginas com links de servi\u00e7os da PGFN if i > 0: y = y + 30 lista_pgs.append(base2 + str(y)) #","title":"Gerando uma lista com as p\u00e1ginas de onde ser\u00e3o \"raspados\" os links dos servi\u00e7os"},{"location":"raspagem/#fazendo-o-scraping-das-informacoes-sobre-cada-servico","text":"# para cada link na lista dos servi\u00e7os, faz o Scraping (raspagem) dos t\u00edtulos e links de cada servi\u00e7o l_lnk = [] # lista para armazenar os links de cada servi\u00e7o l_titulo = [] # lista para armazenas o titulo do servi\u00e7o # for i in lista_pgs: # loop que itera sobre cada p\u00e1gina de onde ser\u00e3o extra\u00eddos os links dos servi\u00e7os try: html = urlopen(i) # acessa a p\u00e1gina que contem os links bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup com o conte\u00fado da p\u00e1gina pg = bs.find('ul', class_='listagem') # encontra a 'tag' <ul> , com classe listagem, que contem os links dos servi\u00e7os itens = pg.find_all('li', class_='item') # encontra a 'tag ' <li> , com classe item, onde est\u00e1 cada link ('tag' <a>) for j in itens: # loop sobre cada item, para extrair o t\u00edtulo, o link e a categoria de cada servi\u00e7o ln1 = j.find('a') # 'tag' <a> que contem o link de cada servi\u00e7o titx = ln1['title'] # atributo 't\u00edtulo' do servi\u00e7o lnx = ln1['href'] # atributo 'href' do servi\u00e7o (link) l_titulo.append(titx) # acrescenta o t\u00edtulo do servi\u00e7o a lista l_lnk.append(lnx) # acrescenta o link para o servi\u00e7o a lista except: #se houver erro n\u00e3o interrompe o programa pass","title":"Fazendo o scraping das informa\u00e7\u00f5es sobre cada servi\u00e7o"},{"location":"raspagem/#utilizando-a-biblioteca-pandas-para-gravar-os-dados-raspados-em-uma-planilha-excel","text":"# grava uma planilha excel com os titulos e os links para cada servi\u00e7o, que ser\u00e3o utilizados para acessar # cada p\u00e1gina do servi\u00e7o e fazer a \"raspagem\" da descri\u00e7\u00e3o do servi\u00e7o dfx = pd.DataFrame({'Servico': l_titulo ,'Link': l_lnk}) dfx.to_excel('./ServicosRFBePGFN.xlsx', index = False)","title":"Utilizando a biblioteca Pandas para gravar os dados \"raspados\" em uma planilha excel"},{"location":"raspagem/#acessa-cada-link-gravado-anteriormente-para-raspar-o-texto-que-descreve-cada-servico","text":"# c\u00f3digo que ir\u00e1 acessar cada link do servi\u00e7o para fazer a \"raspagem\" da texto que descreve o servi\u00e7o links = dfx['Link'] # lista com os links oques = [] # lista onde ser\u00e1 armazenada a descri\u00e7\u00e3o e cada um dos servi\u00e7os for i in range(len(links)): # itera sobre cada link e acessa a p\u00e1gina do servi\u00e7o try: if i < 0: # teste para fazer a raspagem a partir de um item da lista, com 'zero' raspa todos os itens print(\"#\", i) continue html = urlopen(str(links[i])) # acessa a p\u00e1gina do servi\u00e7o print(\"#\", i) # imprime o nr. do item e o link, apenas para visualizar como est\u00e1 o processamento except: continue bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup da p\u00e1gina print(links[i]) div_que = bs.find('div', class_='conteudo') # procura a 'tag' <div> onde est\u00e1 a descri\u00e7\u00e3o do servi\u00e7o texto = div_que.text # texto com a descri\u00e7\u00e3o de cada servi\u00e7o oques.append(texto) # adiciona a descri\u00e7\u00e3o do servi\u00e7o a lista time.sleep(3) # tempo de espera para n\u00e3o sobrecarregar o servidor .gov ##","title":"Acessa cada link gravado anteriormente para \"raspar\" o texto que descreve cada servi\u00e7o"},{"location":"raspagem/#acrescenta-a-planilha-gravada-o-atributo-com-o-texto-que-descreve-o-servico","text":"# # acrescenta a planilha criada anteriormente o atributo que descreve o servi\u00e7o dfx['oQue'] = oques # #Salvando a planilha # dfx.to_excel('./ServicosRFBePGFN_Final.xlsx', index = False) #","title":"Acrescenta a planilha gravada o atributo com o texto que descreve o servi\u00e7o"},{"location":"tratamento/","text":"Coleta e Tratamento dos Dados","title":"Tratamento"},{"location":"tratamento/#coleta-e-tratamento-dos-dados","text":"","title":"Coleta e Tratamento dos Dados"}]}