
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.7">
    
    
      
        <title>Implementacao - ChatBot para Orientações Tributárias e informação sobre os Serviços da Receita Federal</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.cd566b2a.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../extra.css">
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#implementacao" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="ChatBot para Orientações Tributárias e informação sobre os Serviços da Receita Federal" class="md-header__button md-logo" aria-label="ChatBot para Orientações Tributárias e informação sobre os Serviços da Receita Federal" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ChatBot para Orientações Tributárias e informação sobre os Serviços da Receita Federal
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Implementacao
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="ChatBot para Orientações Tributárias e informação sobre os Serviços da Receita Federal" class="md-nav__button md-logo" aria-label="ChatBot para Orientações Tributárias e informação sobre os Serviços da Receita Federal" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    ChatBot para Orientações Tributárias e informação sobre os Serviços da Receita Federal
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Visão Geral
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../metodologia/" class="md-nav__link">
        Metodologia
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../raspagem/" class="md-nav__link">
        Extração dos Textos
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../palavras_chave/" class="md-nav__link">
        Extração das Palavras-chave
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../criando_modelos/" class="md-nav__link">
        Criando os modelos NLP
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../extrai_links/" class="md-nav__link">
        Extraindo Links das Orientações
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../leonino/" class="md-nav__link">
        App Web
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        About
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#implementacao" class="md-nav__link">
    Implementação
  </a>
  
    <nav class="md-nav" aria-label="Implementação">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#extracaoraspagem-dos-dados" class="md-nav__link">
    Extração/Raspagem dos Dados
  </a>
  
    <nav class="md-nav" aria-label="Extração/Raspagem dos Dados">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#importacao-das-bibliotecas-python-necessarias" class="md-nav__link">
    Importação das bibliotecas Python necessárias
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gerando-uma-lista-com-as-paginas-de-onde-serao-raspados-os-links-dos-servicos" class="md-nav__link">
    Gerando uma lista com as páginas de onde serão "raspados" os links dos serviços
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fazendo-o-scraping-das-informacoes-sobre-cada-servico" class="md-nav__link">
    Fazendo o scraping das informações sobre cada serviço
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#utilizando-a-biblioteca-pandas-para-gravar-os-dados-raspados-em-uma-planilha-excel" class="md-nav__link">
    Utilizando a biblioteca Pandas para gravar os dados "raspados" em uma planilha excel
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acessa-cada-link-gravado-anteriormente-para-raspar-o-texto-que-descreve-cada-servico" class="md-nav__link">
    Acessa cada link gravado anteriormente para "raspar" o texto que descreve cada serviço
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#acrescenta-a-planilha-gravada-o-atributo-com-o-texto-que-descreve-o-servico" class="md-nav__link">
    Acrescenta a planilha gravada o atributo com o texto que descreve o serviço
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extracao-das-palavras-chave-dos-servicos" class="md-nav__link">
    Extração das palavras chave dos serviços
  </a>
  
    <nav class="md-nav" aria-label="Extração das palavras chave dos serviços">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#extraindo-as-sentencas-principais-do-texto-oquee-dos-servicos-rfb" class="md-nav__link">
    Extraindo as sentenças principais do texto oQueE dos serviços RFB
  </a>
  
    <nav class="md-nav" aria-label="Extraindo as sentenças principais do texto oQueE dos serviços RFB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#importando-as-bibliotecas" class="md-nav__link">
    Importando as bibliotecas
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extracao-das-palavras-chaves-das-informacoes-de-cada-servico" class="md-nav__link">
    Extração das palavras chaves das informações de cada serviço
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grava-os-servicos-em-uma-planilha" class="md-nav__link">
    Grava os serviços em uma planilha
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#carrega-os-dataframes-com-o-conteudo-das-duas-planilhas" class="md-nav__link">
    Carrega os dataframes com o conteúdo das duas planilhas
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#carrega-bibliotecas-de-nlp-spacy-e-nltk" class="md-nav__link">
    carrega bibliotecas de NLP, spacy e nltk
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenizacao-e-extracao-das-palavras-chave-dos-servicos" class="md-nav__link">
    Tokenização e extração das palavras chave dos serviços
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grava-versao-final-da-planilha-de-servicos" class="md-nav__link">
    Grava versão final da planilha de serviços
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenizacao-e-extracao-das-palavras-chaves-das-frases-de-conversas-comuns" class="md-nav__link">
    Tokenização e extração das palavras chaves das frases de conversas comuns
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#criando-os-modelos-nlp-utilizados-pelo-bot" class="md-nav__link">
    Criando os modelos NLP utilizados pelo Bot
  </a>
  
    <nav class="md-nav" aria-label="Criando os modelos NLP utilizados pelo Bot">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#importando-os-pacotes-gensim" class="md-nav__link">
    importando os pacotes gensim
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lendo-dados-para-treinamento-servicos-e-conversas-comuns" class="md-nav__link">
    Lendo dados para treinamento (serviços e conversas comuns)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modelo-bag-of-wordsbow-criando-um-dicionario-um-corpus-e-um-modelo" class="md-nav__link">
    Modelo Bag of Words(Bow) - Criando um dicionário, um corpus e um modelo
  </a>
  
    <nav class="md-nav" aria-label="Modelo Bag of Words(Bow) - Criando um dicionário, um corpus e um modelo">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#componentes-do-pipeline-para-criar-o-modelo-codigo-acima" class="md-nav__link">
    componentes do pipeline para criar o modelo (código acima)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#criando-uma-matriz-de-similaridades" class="md-nav__link">
    Criando uma matriz de similaridades
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                

  <h1>Implementacao</h1>

<h2 id="implementacao">Implementação</h2>
<h3 id="extracaoraspagem-dos-dados">Extração/Raspagem dos Dados</h3>
<p>A coleta dos dados foi realizada através de código desenvolvido em Python e executados no Jupyter Notebook,
em um notebook de nome 01raspa_servicos.ypnb.</p>
<p>Abaixo mostraremos o código com comentários explicando a lógica.</p>
<h4 id="importacao-das-bibliotecas-python-necessarias">Importação das bibliotecas Python necessárias</h4>
<pre><code class="language-python">#IMPORTANDO AS BIBLIOTECAS NECESSÁRIAS
from bs4 import BeautifulSoup
import time
import requests
import urllib
# importanto a biblioteca pandas
import pandas as pd
from urllib.request import urlopen
#
</code></pre>
<p>A biblioteca BeautifulSoup é que foi utilizada para fazer o Web Scraping(raspagem dos dados).</p>
<h4 id="gerando-uma-lista-com-as-paginas-de-onde-serao-raspados-os-links-dos-servicos">Gerando uma lista com as páginas de onde serão "raspados" os links dos serviços</h4>
<pre><code class="language-python">#
#página base de onde serão extraídos os links para os serviços da RFB no .gov
base1 = 'https://www.gov.br/pt-br/orgaos/secretaria-especial-da-receita-federal-do-brasil?b_start:int='
#página base de onde serão extraídos os links para os serviços da PGFN no .gov
base2 = 'https://www.gov.br/pt-br/orgaos/procuradoria-geral-da-fazenda-nacional?b_start:int='
# gerando lista com as páginas de onde serão extraídos os links para cada serviço existente para os dois órgãos,  RFB e PGFN
lista_pgs = []
y = 0
for i in range(6): # seis páginas com links de serviços da RFB
    if i &gt; 0:
        y = y + 30
    lista_pgs.append(base1 + str(y))
y = 0
for i in range(2): # duas páginas com links de serviços da PGFN
    if i &gt; 0:
        y = y + 30
    lista_pgs.append(base2 + str(y))
#
</code></pre>
<h4 id="fazendo-o-scraping-das-informacoes-sobre-cada-servico">Fazendo o scraping das informações sobre cada serviço</h4>
<pre><code class="language-python"># para cada link na lista dos serviços, faz o Scraping (raspagem) dos títulos e links de cada serviço
l_lnk = [] # lista para armazenar os links de cada serviço
l_titulo = [] # lista para armazenas o titulo do serviço
#
for i in lista_pgs: # loop que itera sobre cada página de onde serão extraídos os links dos serviços
    try:
        html = urlopen(i) # acessa a página que contem os links
        bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup com o conteúdo da página
        pg = bs.find('ul', class_='listagem') # encontra a 'tag' &lt;ul&gt; , com classe listagem, que contem os links dos serviços
        itens = pg.find_all('li', class_='item') # encontra a 'tag ' &lt;li&gt; , com classe item, onde está cada link ('tag' &lt;a&gt;)
        for j in itens: # loop sobre cada item, para extrair o título, o link e a categoria de cada serviço
            ln1 = j.find('a') # 'tag' &lt;a&gt; que contem o link de cada serviço
            titx = ln1['title'] # atributo 'título' do serviço
            lnx = ln1['href'] # atributo 'href' do serviço (link)
            l_titulo.append(titx) # acrescenta o título do serviço a lista
            l_lnk.append(lnx) # acrescenta o link para o serviço a lista
    except: #se houver erro não interrompe o programa
        pass
</code></pre>
<h4 id="utilizando-a-biblioteca-pandas-para-gravar-os-dados-raspados-em-uma-planilha-excel">Utilizando a biblioteca Pandas para gravar os dados "raspados" em uma planilha excel</h4>
<pre><code class="language-python"># grava uma planilha excel com os titulos e os links para cada serviço, que serão utilizados para acessar
# cada página do serviço e fazer a &quot;raspagem&quot; da descrição do serviço
dfx = pd.DataFrame({'Servico': l_titulo ,'Link': l_lnk})
dfx.to_excel('./ServicosRFBePGFN.xlsx', index = False)
</code></pre>
<h4 id="acessa-cada-link-gravado-anteriormente-para-raspar-o-texto-que-descreve-cada-servico">Acessa cada link gravado anteriormente para "raspar" o texto que descreve cada serviço</h4>
<pre><code class="language-python"># código que irá acessar cada link do serviço para fazer a &quot;raspagem&quot; da texto que descreve o serviço
links = dfx['Link'] # lista com os links
oques = [] # lista onde será armazenada a descrição e cada um dos serviços
for i in range(len(links)): # itera sobre cada link e acessa a página do serviço
    try: 
        if i &lt; 0: # teste para fazer a raspagem a partir de um item da lista, com 'zero' raspa todos os itens
            print(&quot;#&quot;, i)
            continue
        html = urlopen(str(links[i])) # acessa a página do serviço
        print(&quot;#&quot;, i) # imprime o nr. do item e o link, apenas para visualizar como está o processamento
    except:
        continue
    bs = BeautifulSoup(html, 'html.parser') # cria o objeto BeautifulSoup da página
    print(links[i])
    div_que = bs.find('div', class_='conteudo') # procura a 'tag' &lt;div&gt; onde está a descrição do serviço
    texto = div_que.text # texto com a descrição de cada serviço
    oques.append(texto) # adiciona a descrição do serviço a lista
    time.sleep(3) # tempo de espera para não sobrecarregar o servidor .gov 
##
</code></pre>
<h4 id="acrescenta-a-planilha-gravada-o-atributo-com-o-texto-que-descreve-o-servico">Acrescenta a planilha gravada o atributo com o texto que descreve o serviço</h4>
<pre><code class="language-python"># 
# acrescenta a planilha criada anteriormente o atributo que descreve o serviço
dfx['oQue'] = oques
#
#Salvando a planilha
#
dfx.to_excel('./ServicosRFBePGFN_Final.xlsx', index = False)
#
</code></pre>
<h3 id="extracao-das-palavras-chave-dos-servicos">Extração das palavras chave dos serviços</h3>
<p><strong>Nome do notebook jupyter: 02extrai_palavras_chave.ypnb</strong>:</p>
<h4 id="extraindo-as-sentencas-principais-do-texto-oquee-dos-servicos-rfb">Extraindo as sentenças principais do texto oQueE dos serviços RFB</h4>
<h5 id="importando-as-bibliotecas">Importando as bibliotecas</h5>
<pre><code class="language-python">#IMPORTANDO AS BIBLIOTECAS NECESSÁRIAS
import  nltk
import time
import pandas as pd
import os
import re
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from string import punctuation
from nltk.probability import FreqDist
from collections import defaultdict
from heapq import nlargest
#
</code></pre>
<h5 id="extracao-das-palavras-chaves-das-informacoes-de-cada-servico">Extração das palavras chaves das informações de cada serviço</h5>
<pre><code class="language-python"># código para verificar as sentenças mais importantes no texto da descrição do serviço, texto do qual
# serão extraídas as palavras chave para utilização nos modelos utilizados pelo chatbot
#
# lista com as stopwords, ou seja, palavras não importantes e que podem ser descartadas 
stopwords1 = stopwords.words('portuguese') + list(punctuation) + ['–']
plx = ['``',&quot;''&quot;] # caracteres de pontuação também indesejáveis
# lê a planilha gerada anteriormente, com os dados dos serviços
df1 = pd.read_excel('./ServicosRFBePGFN_Final.xlsx')
#
chave_lst =  [] # lista para guardar as palavras chave
for i in range(len(df1)): # loop que itera sobre cada serviço para extrair as frases mais importantes do texto
    sentencas = sent_tokenize(df1.iloc[i][2]) # separando o texto em sentenças
    texto = &quot;&quot;
    for j in sentencas: # converte a lista com as sentenças em string
        texto = texto + j + &quot; &quot;
    texto = texto.strip().lower() # transforma o texto em minúsculas
    palavras = word_tokenize(texto) # tokeniza o texto em palavras
    palavras_sem_stopwords = [palavra for palavra in palavras if palavra not in stopwords1] # retira as stopwords do texto
    palavras_sem_stopwords1 = []
    for pal in palavras_sem_stopwords: #retira os caracteres indesejáveis do texto
        if pal not in plx:
            palavras_sem_stopwords1.append(pal)
    palavras_sem_stopwords = palavras_sem_stopwords1
    frequencia = FreqDist(palavras_sem_stopwords) # cria a distribuição de frequência das palavras
    sentencas_importantes = defaultdict(int) # cria dicionário que conterá as sentenças mais importantes
    for k, sentenca in enumerate(sentencas): #calcula a pontuação de cadas sentença, baseado na frequência das palavras
        for palavra in word_tokenize(sentenca.lower()):
            if palavra in frequencia:
                sentencas_importantes[k] += frequencia[palavra]
    # separa as 3 sentenças com maior pontuação
    idx_sentencas_importantes = nlargest(3, sentencas_importantes, sentencas_importantes.get)
    #atribui peso 4 ao título do serviço e peso 3 para a sentença mais importante do texto da descrição do serviço
    str_chave = str(df1.iloc[i][0]+' ')*4 + (sentencas[0]+ ' ')*3 # string com as palavras chave
    for y in sorted(idx_sentencas_importantes): # acrescenta as sentenças importantes na string de palavras chave
        str_chave = str_chave + &quot; &quot; + sentencas[y]
    chave_lst.append(str_chave) # acrescenta a string de palavras chave a lista
#
# para compor a lista das palavras chaves, foi atribuido peso 4 para o título do serviço, peso 4 para a
# frase mais importante e peso 1 para as outras duas frases importantes
#  
</code></pre>
<h5 id="grava-os-servicos-em-uma-planilha">Grava os serviços em uma planilha</h5>
<pre><code class="language-python">df1['texto_chave'] = chave_lst
df1.to_excel('./ServicosRfbFinal.xlsx', index = False)
#
len(df1) # número de serviços da base de dados = 181
</code></pre>
<h5 id="carrega-os-dataframes-com-o-conteudo-das-duas-planilhas">Carrega os dataframes com o conteúdo das duas planilhas</h5>
<pre><code class="language-python">df1 = pd.read_excel('./ServicosRfbFinal.xlsx')
df2 = pd.read_excel('./PerguntasTreinamento.xlsx') # planilha criada manualmente com perguntas/respostas comuns
#
</code></pre>
<h5 id="carrega-bibliotecas-de-nlp-spacy-e-nltk">carrega bibliotecas de NLP, spacy e nltk</h5>
<pre><code class="language-python">import spacy
nlp = spacy.load(&quot;pt_core_news_md&quot;) #carrega o corpus em português do spacy
from nltk.stem import RSLPStemmer
from nltk.tokenize import sent_tokenize, word_tokenize
nltk.download('rslp')
ste = nltk.stem.RSLPStemmer()
</code></pre>
<h5 id="tokenizacao-e-extracao-das-palavras-chave-dos-servicos">Tokenização e extração das palavras chave dos serviços</h5>
<pre><code class="language-python">def troca_acentos(txt): #função que retira os acentos da língua
  transTable = txt.maketrans(&quot;áàãéêíóôõúÁÃÃÉÊÍÓÔÕÚçÇ&quot;, &quot;aaaeeiooouaaaeeioooucc&quot;) 
  txt = txt.translate(transTable)
  return str(txt)
#
#
# função que tokeniza as palavras
#
def tokenizar(texto): # função que tokeniza o texto, e deixa somente palavras significativas
# 
    car_nok = ['SPACE', 'DET','ADP', 'CCONJ','PUNCT','PRON','SCONJ']
    txx = texto
    docxx = nlp(txx)
    txx_lemma = [(token.text,token.orth_, token.lemma_, token.pos_) for token in docxx] #reduz a palavra ao seu Lemma
    pal_xx = ''
    plvxx = &quot;&quot;
    for i in txx_lemma:
        if i[3] not in car_nok and len(i[0]) &gt; 2:
            plvxx = plvxx + ste.stem(i[0]) + ' '
    plvxx = plvxx.strip()
    plvxx = troca_acentos(plvxx)
    return plvxx
# fim funcao tokenizar
#

# função para limpar o texto
def limpa_texto(txt_a_limpar): # transforma em minúsculas e retira espa;os iniciais e finais

    txt_ok = txt_a_limpar.lower()
    txt_ok = txt_ok.strip()
    return txt_ok
#
# fim da funcao limpa_texto
#
#
# cria planilha com as palavras chave dos serviços
print(&quot;***inicio tokenização dos serviços***&quot;)
plvx1 = []
# loop para percorrer o dataframe e extrair as palavras chave
for j in range(len(df1)):
    # composição do texto = 3 vezes palavras do título descrevendo o serviço
    # supõe-se serem as palavras mais importantes, por isso deu-se a elas o peso 5
    # as palavras explicando o que é o serviço (oQue) também serão extraídas
    txt = df1.iloc[j][3]
    txt_token = tokenizar(limpa_texto(txt))
    plvx1.append(txt_token)
# acrescenta coluna com as palavras chave ao dataframe
df1['chaves_serv'] = plvx1
#

#
print(&quot;#### fim tokenização dos serviços ####&quot;)
#
</code></pre>
<h5 id="grava-versao-final-da-planilha-de-servicos">Grava versão final da planilha de serviços</h5>
<pre><code class="language-python"># grava as palavras chave na planilha de serviços
df1.to_excel('./ServicosRfbFinal2.xlsx', index = False)
</code></pre>
<h5 id="tokenizacao-e-extracao-das-palavras-chaves-das-frases-de-conversas-comuns">Tokenização e extração das palavras chaves das frases de conversas comuns</h5>
<pre><code class="language-python">print(&quot;*** inicio tokenização das conversas ***&quot;)
# cria planilha com as palavras chave dos serviços
plvx1 = []
# loop para percorrer o dataframe e extrair as palavras chave
for j in range(len(df2)):
    # composição do texto = 3 vezes palavras do título descrevendo o serviço
    # supõe-se serem as palavras mais importantes, por isso deu-se a elas o peso 5
    # as palavras explicando o que é o serviço (oQue) também serão extraídas
    txt = df2.iloc[j][0] + ' ' 
    txt_token = tokenizar(limpa_texto(txt))
    plvx1.append(txt_token)
# acrescenta coluna com as palavras chave ao dataframe
df2['chaves_serv'] = plvx1
#
print(&quot;#### fim tokenização das conversas ####&quot;)
</code></pre>
<h3 id="criando-os-modelos-nlp-utilizados-pelo-bot">Criando os modelos NLP utilizados pelo Bot</h3>
<p><strong>Continuação do  notebook jupyter: 02extrai_palavras_chave.ypnb</strong>:</p>
<h4 id="importando-os-pacotes-gensim">importando os pacotes gensim</h4>
<pre><code class="language-python">#!pip install gensim
import gensim
import gensim.downloader as api
from gensim.models import TfidfModel
from gensim.corpora import Dictionary
from gensim import similarities
</code></pre>
<h4 id="lendo-dados-para-treinamento-servicos-e-conversas-comuns">Lendo dados para treinamento (serviços e conversas comuns)</h4>
<pre><code class="language-python">#
df1 = pd.read_excel('./ServicosRfbFinal2.xlsx')
#
servicos = []
for i in range(len(df1)):
    servicos.append(str(df1.iloc[i][4]))
ids = [i for i in range(len(df1))]
dataset1 = pd.DataFrame({'id': ids,'desc': servicos})
#
df2 = pd.read_excel('./PerguntasTreinamento2.xlsx')
#
conversas = []
for i in range(len(df2)):
    conversas.append(str(df2.iloc[i][3]))
ids = [i for i in range(len(df2))]
dataset2 = pd.DataFrame({'id': ids,'desc': conversas})
</code></pre>
<h3 id="modelo-bag-of-wordsbow-criando-um-dicionario-um-corpus-e-um-modelo">Modelo Bag of Words(Bow) - Criando um dicionário, um corpus e um modelo</h3>
<pre><code class="language-python"># Transforma a descricao do servico em tokens de palavras
input_tokens = [d.split() for d in dataset1['desc'].values]

# Criando um dicionario com os textos do dataset
dct1 = Dictionary(input_tokens)

# Converte corpus para o formato BOW (Bag Of Words)
corpus1 = [dct1.doc2bow(line) for line in input_tokens]

# Cria um modelo TF-IDF
model1 = TfidfModel(corpus1)
#
# Transforma as conversas de treinamento em tokens de palavras
input_tokens = [d.split() for d in dataset2['desc'].values]

# Criando um dicionario com os textos do dataset
dct2 = Dictionary(input_tokens)

# Converte corpus para o formato BOW (Bag Of Words)
corpus2 = [dct2.doc2bow(line) for line in input_tokens]

# Cria um modelo TF-IDF
model2 = TfidfModel(corpus2)
#
# Salvando os modelos e os dicionários para uso do Chatbot
#
dct1.save('dct1')
model1.save('model1')
dct2.save('dct2')
model2.save('model2')
</code></pre>
<p><strong>OBSERVAÇÕES</strong></p>
<h4 id="componentes-do-pipeline-para-criar-o-modelo-codigo-acima">componentes do pipeline para criar o modelo (código acima)</h4>
<ul>
<li>
<p><strong>input_tokens</strong> -&gt; separa as palavras de cada texto;</p>
</li>
<li>
<p><strong>dct</strong> -&gt;  lista cada palavra dos textos em um dicionario;</p>
</li>
<li>
<p><strong>corpus</strong> -&gt; faz um mapeamento de cada texto com os ids do dicionario (id da palavra, quantidade de ocorrencias);</p>
</li>
<li>
<p><strong>model</strong> -&gt; é o transformer responsavel por traduzir o corpus em um TF-IDF</p>
</li>
</ul>
<h4 id="criando-uma-matriz-de-similaridades">Criando uma matriz de similaridades</h4>
<p>Até aqui fizemos um pipeline que transformou textos em matrizes TF-IDF e agora podemos comparar duas TF-IDFs e verificar sua 
similaridade. Então iremos criar duas matrizes de similaridade com todos os textos dos Serviços e das conversar comuns, 
que chamaremos de index1 e index2. Com as matrizes criadas poderemos comparar o texto digitado pelo usuário com os textos
existentes em nossa base de dados.</p>
<pre><code class="language-python"># Cria a matriz de similaridade dos Serviços
index1 = similarities.SparseMatrixSimilarity(model1[corpus1], num_features=len(dct1))
# Salva a matriz para uso no pipeline principal
index1.save('index1')
#
# Cria a matriz de similaridade das conversas comuns
index2 = similarities.SparseMatrixSimilarity(model2[corpus2], num_features=len(dct2))
# Salva a matriz para uso no pipeline principal
index2.save('index2')

</code></pre>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.22074ed6.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.01de222e.min.js"></script>
      
    
  </body>
</html>